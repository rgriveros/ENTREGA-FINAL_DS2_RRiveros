{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNt1LKibjwd+xFMsSG77qGf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgriveros/ENTREGA-FINAL_DS2_RRiveros/blob/main/EntregaFinal_DS2_RRiveros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción de alcanzamiento de etapa avanzada en proyectos mineros argentinos\n",
        "\n",
        "## Entrega final — Proyecto DS2\n",
        "\n",
        "## Autor: Gabriel Riveros Lobos\n",
        "\n",
        "### Rol: Process improvement technician · Data analyst (freelance)\n",
        "\n",
        "### Repositorio / Notebook: EntregaFinal_DS2_RRiveros.ipynb\n",
        "\n",
        "### Artefactos entregados:\n",
        "data/artifacts/{preproc_fitted.joblib, df_transformed.parquet, split_indices.joblib, model_final.joblib, final_metrics.csv, shap_summary.png, shap_feature_table.csv, best_params_lightgbm.json}\n",
        "\n",
        "### Métrica primaria: PR AUC (Average Precision)\n",
        "\n",
        "### Modelo final seleccionado: LightGBM (reentrenado sobre train+val)\n",
        "\n",
        "Fecha de entrega: 13/11/2025\n",
        "\n",
        "\n",
        "Resumen de la entrega:\n",
        "Notebook reproducible que descarga y versiona datos, aplica limpieza no destructiva y aplicada, construye pipeline de preprocesado auditable, crea splits reproducibles, compara modelos baseline y avanzados, selecciona y persiste LightGBM final, y entrega análisis de interpretabilidad con SHAP.\n",
        "\n",
        "## Abstracto\n",
        "Este trabajo desarrolla y entrega un flujo reproducible para predecir la probabilidad de que un proyecto minero público en Argentina alcance una etapa avanzada. Partiendo de datos públicos obtenidos por CKAN, se aplicó una limpieza no destructiva (generación de columnas *_clean), control de calidad y versionado de artefactos. La etapa de preprocesado construyó un pipeline auditable (ColumnTransformer + Pipeline) que prioriza columnas limpias y documenta transformaciones; las features resultantes se persistieron en formato parquet para modelado.\n",
        "\n",
        "Para evaluación se definió PR AUC (Average Precision) como métrica primaria, dada la naturaleza desbalanceada del problema y la necesidad operacional de priorizar un top-k de proyectos. Se compararon modelos baseline y modelos avanzados; LightGBM superó los baselines en PR AUC y métricas complementarias (AUC ROC, Precision@k, Brier score). El modelo final fue reentrenado sobre train+val con semilla fija y persistido junto al pipeline de preprocesado. Se incluyeron comprobaciones de calibración y una propuesta de umbrales operativos (Precision@k) para facilitar la adopción práctica.\n",
        "\n",
        "La interpretación se realizó con SHAP: se exportó una tabla de importancias y dirección de efecto por feature, junto con visualizaciones summary y ejemplos locales para casos representativos. Se documentan limitaciones importantes: tamaño limitado de la muestra, clases con baja frecuencia que requieren decisiones sobre agrupamiento o muestreo, y dependencia de la calidad de variables originales (posibles columnas constantes o mal codificadas). Se incluyen recomendaciones inmediatas para producción: persistir preproc + modelo en un endpoint con validación de inputs, monitorear deriva y métricas operativas (Precision@k, distribución de probabilidades, Brier score), y priorizar mejora de calidad y trazabilidad de las variables críticas.\n",
        "\n",
        "El repositorio entregado contiene instrucciones paso a paso para replicar el experimento en una sesión limpia, los artefactos necesarios para auditoría y despliegue, y un README inicial que resume decisiones clave y cómo reproducir la evaluación y generación de reportes."
      ],
      "metadata": {
        "id": "Gb-a7bzFMUBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Requisitos, entorno y reproducibilidad\n",
        "\n",
        "Objetivo: dejar configurado el entorno mínimo necesario para que el notebook sea reproducible y ejecutable en Colab y en cualquier máquina local con Python. Esto incluye:\n",
        "- Declarar y fijar la semilla global (RANDOM_STATE).\n",
        "- Importar librerías clave y mostrar versiones para trazabilidad.\n",
        "- Definir y crear la estructura de carpetas usada por el proyecto.\n",
        "- Generar archivos iniciales: `README_colab.txt` y `data/sample_input.csv` (muestra vacía), listos para subir al repo.\n",
        "\n"
      ],
      "metadata": {
        "id": "hK1dh2ssejCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 02: Imports, reproducibilidad y paths (definir al inicio)\n",
        "import sys\n",
        "import os\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Reproducibilidad: usar RNG explícito donde sea posible\n",
        "RANDOM_STATE = 42\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "\n",
        "# Estructura de carpetas (usar variables únicas)\n",
        "DATA_DIR = Path(\"data\")\n",
        "ART = DATA_DIR / \"artifacts\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Mostrar versiones clave (salida compacta)\n",
        "print(\"Python:\", sys.version.splitlines()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(f\"NumPy: {np.__version__}  |  Pandas: {pd.__version__}  |  Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"Matplotlib: {matplotlib.__version__}  |  Seaborn: {sns.__version__}  |  Joblib: {joblib.__version__}\")\n",
        "print(\"RANDOM_STATE:\", RANDOM_STATE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsOM47v1rsfG",
        "outputId": "b00a11c7-3daa-4a19-88b4-e6224c9db8b4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "NumPy: 2.0.2  |  Pandas: 2.2.2  |  Scikit-learn: 1.6.1\n",
            "Matplotlib: 3.10.0  |  Seaborn: 0.13.2  |  Joblib: 1.5.2\n",
            "RANDOM_STATE: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Imports, seed, paths y creación de estructura\n",
        "import sys, os, platform\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib, matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Semilla reproducible\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Imprimir versiones clave para reproducibilidad\n",
        "print(\"Python:\", sys.version.splitlines()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Matplotlib:\", matplotlib.__version__)\n",
        "print(\"Seaborn:\", sns.__version__)\n",
        "print(\"Scikit-learn:\", sklearn.__version__)\n",
        "print(\"Joblib:\", joblib.__version__)\n",
        "print(\"RANDOM_STATE set to\", RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC7tq5LSevU2",
        "outputId": "2d007e40-19bd-4451-cc9e-5970d1b0608c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "NumPy: 2.0.2\n",
            "Pandas: 2.2.2\n",
            "Matplotlib: 3.10.0\n",
            "Seaborn: 0.13.2\n",
            "Scikit-learn: 1.6.1\n",
            "Joblib: 1.5.2\n",
            "RANDOM_STATE set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Carga de Datos y chequeo rápido EDA\n",
        "\n",
        "Objetivo:\n",
        "\n",
        "Esta celda descarga el dataset público desde la API CKAN de datos.gob.ar (package_search → package_show → recurso CSV/XLSX), guarda el archivo en la carpeta local data/ del repositorio y genera un snapshot local para trazabilidad. No requiere acceso a Google Drive. Variables editables: QUERY, ROWS, VERIFY_SSL. Mantener VERIFY_SSL = True; usar False solo si el entorno falla por certificado (documentar el uso). Dependencias: requests, pandas, openpyxl (si hay Excel).\n"
      ],
      "metadata": {
        "id": "Xgjtjt-DRa6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Descarga por API CKAN -> guarda en ./data/ -> lee y snapshot\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# ---------- EDITAR SOLO ESTAS VARIABLES ----------\n",
        "CKAN_BASE = \"https://datos.gob.ar/api/3/action\"\n",
        "QUERY = \"Proyectos mineros metalíferos y de litio\"\n",
        "ROWS = 5\n",
        "VERIFY_SSL = False   # True recomendado; False solo si hay error certificado y lo documentás\n",
        "# -------------------------------------------------\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def ck_search_and_download(query, rows=5, verify_ssl=True, retries=2, backoff=2):\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            r = requests.get(f\"{CKAN_BASE}/package_search\", params={\"q\": query, \"rows\": rows}, timeout=30, verify=verify_ssl)\n",
        "            r.raise_for_status()\n",
        "            results = r.json().get(\"result\", {}).get(\"results\", [])\n",
        "            if not results:\n",
        "                raise FileNotFoundError(f\"No se encontraron paquetes para la query: '{query}'\")\n",
        "            first = results[0]\n",
        "            pkg_id = first.get(\"id\") or first.get(\"name\")\n",
        "            r2 = requests.get(f\"{CKAN_BASE}/package_show\", params={\"id\": pkg_id}, timeout=30, verify=verify_ssl)\n",
        "            r2.raise_for_status()\n",
        "            pkg = r2.json().get(\"result\", {})\n",
        "            resources = pkg.get(\"resources\", []) or []\n",
        "            for res in resources:\n",
        "                url = (res.get(\"url\") or \"\").strip()\n",
        "                if url.lower().endswith((\".csv\", \".xlsx\", \".xls\")):\n",
        "                    return pkg, url\n",
        "            raise FileNotFoundError(\"No se encontró recurso CSV/XLSX en el paquete seleccionado.\")\n",
        "        except (requests.RequestException, ValueError) as e:\n",
        "            if attempt < retries:\n",
        "                time.sleep(backoff * (attempt + 1))\n",
        "                continue\n",
        "            raise RuntimeError(f\"Fallo al consultar CKAN: {e}\")\n",
        "\n",
        "pkg_meta, res_url = ck_search_and_download(QUERY, ROWS, VERIFY_SSL)\n",
        "\n",
        "dst = DATA_DIR / (\"dataset_minero.csv\" if res_url.lower().endswith(\".csv\") else \"dataset_minero.xlsx\")\n",
        "with requests.get(res_url, stream=True, timeout=120, verify=VERIFY_SSL) as resp:\n",
        "    resp.raise_for_status()\n",
        "    with open(dst, \"wb\") as f:\n",
        "        for chunk in resp.iter_content(8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "if dst.suffix.lower() == \".csv\":\n",
        "    df = pd.read_csv(dst, low_memory=False, encoding=\"utf-8\")\n",
        "else:\n",
        "    df = pd.read_excel(dst, engine=\"openpyxl\")\n",
        "\n",
        "# Guardar snapshot para trazabilidad\n",
        "snapshot = DATA_DIR / \"data_snapshot_head.csv\"\n",
        "df.head(200).to_csv(snapshot, index=False)\n",
        "\n",
        "# Salida mínima y reproducible\n",
        "print(\"Package:\", pkg_meta.get(\"title\") or pkg_meta.get(\"name\"))\n",
        "print(\"Fuente (URL):\", res_url)\n",
        "print(\"Guardado en:\", dst)\n",
        "print(\"Filas:\", df.shape[0], \"| Columnas:\", df.shape[1])\n",
        "print(df.isna().sum().sort_values(ascending=False).head(20).to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6TBvK35A2lC",
        "outputId": "179b9afc-36d3-4dd7-bed5-285785f7d399"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'datos.gob.ar'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'datos.gob.ar'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.mecon.gob.ar'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.economia.gob.ar'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package: Cartera de Proyectos Mineros en Argentina del SIACAM\n",
            "Fuente (URL): https://www.mecon.gob.ar/dataset/Cartera-de-Proyectos-Mineros-Metaliferos-y-Litio-del-SIACAM.xlsx\n",
            "Guardado en: data/dataset_minero.xlsx\n",
            "Filas: 325 | Columnas: 17\n",
            "Unnamed: 16          324\n",
            "PORCENTAJE (3°)      235\n",
            "ORIGEN (2°)          233\n",
            "ORIGEN (3°)          232\n",
            "CONTROLANTE (3°)     232\n",
            "CONTROLANTE (2°)     231\n",
            "PORCENTAJE (2°)      229\n",
            "PORCENTAJE (1°)      122\n",
            "ORIGEN (1°)          121\n",
            "CONTROLANTE (1°)      97\n",
            "N°                     0\n",
            "PROVINCIA              0\n",
            "ESTADO                 0\n",
            "LATITUD                0\n",
            "NOMBRE                 0\n",
            "LONGITUD               0\n",
            "MINERAL PRINCIPAL      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardando metadatos del paquete (id, title, url, fecha de descarga) para trazabilidad reproducible"
      ],
      "metadata": {
        "id": "Nx2b1ltSD-e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar metadatos del paquete\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "now_utc = datetime.now(timezone.utc).isoformat()\n",
        "meta[\"downloaded_at\"] = now_utc\n",
        "\n",
        "\n",
        "META_PATH = Path(\"data\")\n",
        "META_PATH.mkdir(parents=True, exist_ok=True)\n",
        "OUT = META_PATH / \"dataset_metadata.json\"\n",
        "\n",
        "def safe_str(x):\n",
        "    try:\n",
        "        return str(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "meta = {\n",
        "    \"package_id\": safe_str(pkg_meta.get(\"id\") if isinstance(pkg_meta, dict) else pkg_meta),\n",
        "    \"package_title\": safe_str(pkg_meta.get(\"title\") if isinstance(pkg_meta, dict) else pkg_meta),\n",
        "    \"resource_url\": safe_str(res_url),\n",
        "    \"downloaded_at\": datetime.now(timezone.utc).isoformat()\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "    print(\"Metadatos guardados en:\", OUT)\n",
        "except Exception as e:\n",
        "    print(\"Error al guardar metadatos:\", type(e).__name__, \"-\", e)\n",
        "    print(\"Contenido meta (fallback):\", {k: v for k, v in meta.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLJWgf6uDsEf",
        "outputId": "68951f0f-1e1e-4dcf-ce00-41bc98f23d0b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadatos guardados en: data/dataset_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks5oEBaXEFIe",
        "outputId": "4f8b6aa8-efa2-4853-cae8-33def7adb5bf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N°                     int64\n",
            "NOMBRE                object\n",
            "LATITUD              float64\n",
            "LONGITUD             float64\n",
            "MINERAL PRINCIPAL     object\n",
            "PROVINCIA             object\n",
            "ESTADO                object\n",
            "CONTROLANTE (1°)      object\n",
            "PORCENTAJE (1°)      float64\n",
            "ORIGEN (1°)           object\n",
            "CONTROLANTE (2°)      object\n",
            "ORIGEN (2°)           object\n",
            "PORCENTAJE (2°)       object\n",
            "CONTROLANTE (3°)      object\n",
            "PORCENTAJE (3°)       object\n",
            "ORIGEN (3°)           object\n",
            "Unnamed: 16           object\n",
            "dtype: object\n",
            "   N°             NOMBRE  LATITUD  LONGITUD MINERAL PRINCIPAL  PROVINCIA  \\\n",
            "0   1  20 de septiembre   -24.896   -68.136            Hierro      Salta   \n",
            "1   2           Acazoque  -24.291   -66.378             Plomo      Salta   \n",
            "2   3   Acoite/Hornillos  -22.305   -65.107             Plomo      Salta   \n",
            "3   4              Adamo  -41.162   -68.505               Oro  Río Negro   \n",
            "4   5      Aguas Amargas  -24.685   -66.903             Cobre      Salta   \n",
            "\n",
            "                ESTADO        CONTROLANTE (1°)  PORCENTAJE (1°) ORIGEN (1°)  \\\n",
            "0  Exploración inicial        Diego Ruben Omar              NaN         NaN   \n",
            "1  Exploración inicial             Nuñez Ramon              NaN         NaN   \n",
            "2  Exploración inicial  Rubiolo Daniel Gerardo              NaN         NaN   \n",
            "3          Prospección  Valcheta Exploraciones              1.0   Argentina   \n",
            "4  Exploración inicial     Lithium S Corp. S.A              NaN         NaN   \n",
            "\n",
            "  CONTROLANTE (2°) ORIGEN (2°) PORCENTAJE (2°) CONTROLANTE (3°)  \\\n",
            "0              NaN         NaN             NaN              NaN   \n",
            "1              NaN         NaN             NaN              NaN   \n",
            "2              NaN         NaN             NaN              NaN   \n",
            "3              NaN         NaN             NaN              NaN   \n",
            "4              NaN         NaN             NaN              NaN   \n",
            "\n",
            "  PORCENTAJE (3°) ORIGEN (3°) Unnamed: 16  \n",
            "0             NaN         NaN         NaN  \n",
            "1             NaN         NaN         NaN  \n",
            "2             NaN         NaN         NaN  \n",
            "3             NaN         NaN         NaN  \n",
            "4             NaN         NaN         NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observación rápida sobre la salida\n",
        "La lectura funcionó, pero hay columnas que deberían ser numéricas (p. ej. las columnas PORCENTAJE (...)) y aparecen como object — eso suele pasar por valores mezclados (cadenas, comas decimales, símbolos % o celdas vacías/extrañas).\n",
        "\n",
        "Hay una columna Unnamed: 16 vacía en su mayoría — probable columna residual del Excel que conviene eliminar.\n",
        "\n",
        "Latitud/Longitud ya son float64 (bien).\n",
        "\n",
        "Antes de cualquier análisis sería conveniente:\n",
        "1) inspeccionar las celdas “problemáticas”,\n",
        "2) normalizar formatos numéricos,\n",
        "3) convertir a tipos correctos\n",
        "4) registrar los cambios (snapshot + metadata)."
      ],
      "metadata": {
        "id": "Nc6JhJ3vGK4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Limpieza No Destructiva"
      ],
      "metadata": {
        "id": "8tcdhb0UGx-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 04: Limpieza no destructiva y creación idempotente de columnas *_clean\n",
        "import re\n",
        "from datetime import timezone\n",
        "import shutil\n",
        "\n",
        "RAW_FN = DATA_DIR / \"dataset_minero.xlsx\"\n",
        "OUT_CSV = DATA_DIR / \"dataset_minero_clean.csv\"\n",
        "SNAPSHOT_CSV = DATA_DIR / \"data_snapshot_head_clean.csv\"\n",
        "META_F = DATA_DIR / \"dataset_metadata.json\"\n",
        "\n",
        "# Usar df ya cargado por la celda anterior\n",
        "if 'df' not in globals():\n",
        "    if RAW_FN.exists():\n",
        "        df = pd.read_excel(RAW_FN, engine=\"openpyxl\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Archivo raw no encontrado: {RAW_FN}\")\n",
        "\n",
        "def clean_numeric_value(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    if s in (\"\", \"-\", \"--\"):\n",
        "        return None\n",
        "    s = s.replace(\"\\xa0\", \"\").replace(\" \", \"\")\n",
        "    s = re.sub(r\"[^\\d\\-,.\\+eE%]\", \"\", s)\n",
        "    if s == \"\" or s in (\"-\", \"--\"):\n",
        "        return None\n",
        "    is_pct = s.endswith(\"%\")\n",
        "    if is_pct:\n",
        "        s = s[:-1]\n",
        "    # manejar separadores mixtos\n",
        "    if \",\" in s and \".\" in s:\n",
        "        if s.rfind(\",\") > s.rfind(\".\"):\n",
        "            s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
        "        else:\n",
        "            s = s.replace(\",\", \"\")\n",
        "    else:\n",
        "        s = s.replace(\",\", \".\")\n",
        "    try:\n",
        "        v = float(s)\n",
        "        if is_pct:\n",
        "            v = v / 100.0\n",
        "        return v\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# detectar columnas con 'PORCENTAJE' y crear columnas *_clean solo si no existen\n",
        "pct_cols = [c for c in df.columns if \"PORCENTAJE\" in str(c).upper()]\n",
        "conversion_report = {}\n",
        "for c in pct_cols:\n",
        "    clean_name = c if c.endswith(\"_clean\") else f\"{c}_clean\"\n",
        "    if clean_name in df.columns:\n",
        "        conversion_report[c] = {\"status\": \"exists\"}\n",
        "        continue\n",
        "    before = int(df[c].notna().sum())\n",
        "    df[clean_name] = df[c].map(clean_numeric_value)\n",
        "    after = int(df[clean_name].notna().sum())\n",
        "    conversion_report[c] = {\"before_nonnull\": before, \"after_numeric\": after}\n",
        "\n",
        "# eliminar columnas totalmente vacías (Unnamed) y columnas all-NaN\n",
        "unnamed = [c for c in df.columns if str(c).lower().startswith(\"unnamed\") and df[c].dropna().eq(\"\").all()]\n",
        "allnan = [c for c in df.columns if df[c].isna().all()]\n",
        "drop_candidates = sorted(set(unnamed + allnan))\n",
        "\n",
        "if drop_candidates:\n",
        "    df = df.drop(columns=drop_candidates, errors='ignore')\n",
        "\n",
        "# guardar outputs no destructivos\n",
        "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "df.head(200).to_csv(SNAPSHOT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# actualizar metadata\n",
        "meta = json.loads((DATA_DIR / \"dataset_metadata.json\").read_text(encoding=\"utf-8\")) if (DATA_DIR / \"dataset_metadata.json\").exists() else {}\n",
        "meta.update({\n",
        "    \"clean_applied_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"conversion_report\": conversion_report,\n",
        "    \"dropped_columns\": drop_candidates\n",
        "})\n",
        "(DATA_DIR / \"dataset_metadata.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Limpieza aplicada (no destructiva). Archivos creados:\")\n",
        "print(\"-\", OUT_CSV)\n",
        "print(\"-\", SNAPSHOT_CSV)\n",
        "print(\"-\", DATA_DIR / \"dataset_metadata.json\")\n",
        "print(\"Conversion report (ej):\", list(conversion_report.items())[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXSM_jnCtF7e",
        "outputId": "39f7a751-9dfc-45f7-af49-65cdb551ff48"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpieza aplicada (no destructiva). Archivos creados:\n",
            "- data/dataset_minero_clean.csv\n",
            "- data/data_snapshot_head_clean.csv\n",
            "- data/dataset_metadata.json\n",
            "Conversion report (ej): [('PORCENTAJE (1°)', {'before_nonnull': 203, 'after_numeric': 203}), ('PORCENTAJE (2°)', {'before_nonnull': 96, 'after_numeric': 30}), ('PORCENTAJE (3°)', {'before_nonnull': 90, 'after_numeric': 4})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1785576022.py:75: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"clean_applied_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 04.1: Aplicar *_clean sobre columnas originales (hacer backup antes)\n",
        "BACKUP_RAW_CSV = DATA_DIR / \"dataset_minero_raw_backup.csv\"\n",
        "OUT_APPLIED = DATA_DIR / \"dataset_minero_clean_applied.csv\"\n",
        "\n",
        "# cargar el clean intermedio si existe\n",
        "candidate_clean = DATA_DIR / \"dataset_minero_clean.csv\"\n",
        "if candidate_clean.exists():\n",
        "    df = pd.read_csv(candidate_clean, low_memory=False)\n",
        "elif RAW_FN.exists():\n",
        "    df = pd.read_excel(RAW_FN, engine=\"openpyxl\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No se encontró dataset intermedio ni raw para aplicar los cambios.\")\n",
        "\n",
        "# identificar columnas *_clean y mapear a originales\n",
        "clean_cols = [c for c in df.columns if isinstance(c, str) and c.endswith(\"_clean\")]\n",
        "mapping = {}\n",
        "for c in clean_cols:\n",
        "    orig = c[:-6]\n",
        "    if orig in df.columns:\n",
        "        mapping[orig] = c\n",
        "\n",
        "if not mapping:\n",
        "    raise RuntimeError(\"No se encontraron columnas *_clean que correspondan a originales. Nada que aplicar.\")\n",
        "\n",
        "# crear backup y aplicar reemplazo (sobrescribir originales con clean)\n",
        "df.to_csv(BACKUP_RAW_CSV, index=False, encoding=\"utf-8\")\n",
        "replaced = {}\n",
        "for orig, clean in mapping.items():\n",
        "    before_nonnull = int(df[orig].notna().sum()) if orig in df.columns else 0\n",
        "    df[orig] = df[clean]\n",
        "    after_nonnull = int(df[orig].notna().sum())\n",
        "    replaced[orig] = {\"clean_column\": clean, \"before_nonnull\": before_nonnull, \"after_nonnull\": after_nonnull}\n",
        "\n",
        "df.to_csv(OUT_APPLIED, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# actualizar metadata\n",
        "meta = json.loads((DATA_DIR / \"dataset_metadata.json\").read_text(encoding=\"utf-8\")) if (DATA_DIR / \"dataset_metadata.json\").exists() else {}\n",
        "meta.update({\n",
        "    \"applied_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"backup_raw_csv\": str(BACKUP_RAW_CSV),\n",
        "    \"applied_file\": str(OUT_APPLIED),\n",
        "    \"columns_replaced\": replaced\n",
        "})\n",
        "(DATA_DIR / \"dataset_metadata.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Dataset final guardado en:\", OUT_APPLIED)\n",
        "print(\"Backup raw en:\", BACKUP_RAW_CSV)\n",
        "print(\"Replaced sample:\", list(replaced.items())[:4])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwVasl9MuAZz",
        "outputId": "5150f843-19c7-4c1b-8951-e7ac8954e254"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset final guardado en: data/dataset_minero_clean_applied.csv\n",
            "Backup raw en: data/dataset_minero_raw_backup.csv\n",
            "Replaced sample: [('PORCENTAJE (1°)', {'clean_column': 'PORCENTAJE (1°)_clean', 'before_nonnull': 203, 'after_nonnull': 203}), ('PORCENTAJE (2°)', {'clean_column': 'PORCENTAJE (2°)_clean', 'before_nonnull': 96, 'after_nonnull': 30}), ('PORCENTAJE (3°)', {'clean_column': 'PORCENTAJE (3°)_clean', 'before_nonnull': 90, 'after_nonnull': 4})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-110352426.py:39: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"applied_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 04.2 aplicar a columnas PORCENTAJE y crear columnas *_clean (no sobreescribir)\n",
        "conversion_report = {}\n",
        "for c in pct_cols:\n",
        "    before = int(df[c].notna().sum())\n",
        "    cleaned = df[c].map(clean_numeric_value)\n",
        "    after = int(cleaned.notna().sum())\n",
        "    df[c + \"_clean\"] = cleaned\n",
        "    conversion_report[c] = {\"before_nonnull\": before, \"after_numeric\": after}\n",
        "\n",
        "# 4) opcional: aplicar a otras columnas object que el usuario confirme (aquí no se aplica por defecto)\n",
        "# Si querés convertir columnas adicionales, agrégalas a cols_to_convert\n",
        "cols_to_convert = []  # p. ej. [\"PORCENTAJE (2°)\"] si no fue detectada por nombre\n",
        "for c in cols_to_convert:\n",
        "    if c in df.columns:\n",
        "        before = int(df[c].notna().sum())\n",
        "        cleaned = df[c].map(clean_numeric_value)\n",
        "        after = int(cleaned.notna().sum())\n",
        "        df[c + \"_clean\"] = cleaned\n",
        "        conversion_report[c] = {\"before_nonnull\": before, \"after_numeric\": after}\n",
        "\n",
        "# 04.3) eliminar columnas vacías tipo Unnamed si están totalmente vacías\n",
        "unnamed = [c for c in df.columns if str(c).lower().startswith(\"unnamed\") and df[c].dropna().eq(\"\").all()]\n",
        "# además columnas con todos NaN\n",
        "allnan = [c for c in df.columns if df[c].isna().all()]\n",
        "drop_candidates = sorted(set(unnamed + allnan))\n",
        "if drop_candidates:\n",
        "    print(\"Columnas vacías a dropear (no destructivo todavía):\", drop_candidates)\n",
        "    df = df.drop(columns=drop_candidates, errors='ignore')"
      ],
      "metadata": {
        "id": "Qrfjm8r8HDuW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04.4) guardar clean CSV y snapshot head\n",
        "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "df.head(200).to_csv(SNAPSHOT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# 04.5) metadata ampliada\n",
        "meta = {\n",
        "    \"package_title\": pkg_meta.get(\"title\") if 'pkg_meta' in globals() else None,\n",
        "    \"package_id\": pkg_meta.get(\"id\") if 'pkg_meta' in globals() else None,\n",
        "    \"resource_url\": res_url if 'res_url' in globals() else None,\n",
        "    \"raw_file\": str(RAW_FN),\n",
        "    \"clean_file\": str(OUT_CSV),\n",
        "    \"snapshot_file\": str(SNAPSHOT_CSV),\n",
        "    \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\",\"Z\"),\n",
        "    \"conversion_report\": conversion_report,\n",
        "    \"dropped_columns\": drop_candidates\n",
        "}\n",
        "with open(META_F, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Limpieza aplicada (no destructiva). Archivos creados:\")\n",
        "print(\" - Clean dataset:\", OUT_CSV)\n",
        "print(\" - Snapshot head:\", SNAPSHOT_CSV)\n",
        "print(\" - Metadata:\", META_F)\n",
        "print(\"\\nResumen conversion_report (porcentaje columnas):\")\n",
        "print(json.dumps(conversion_report, indent=2, ensure_ascii=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c7kWFwHHIn",
        "outputId": "c1e38e16-4af6-4ec7-9e12-95a0b42b9a60"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpieza aplicada (no destructiva). Archivos creados:\n",
            " - Clean dataset: data/dataset_minero_clean.csv\n",
            " - Snapshot head: data/data_snapshot_head_clean.csv\n",
            " - Metadata: data/dataset_metadata.json\n",
            "\n",
            "Resumen conversion_report (porcentaje columnas):\n",
            "{\n",
            "  \"PORCENTAJE (1°)\": {\n",
            "    \"before_nonnull\": 203,\n",
            "    \"after_numeric\": 203\n",
            "  },\n",
            "  \"PORCENTAJE (2°)\": {\n",
            "    \"before_nonnull\": 30,\n",
            "    \"after_numeric\": 30\n",
            "  },\n",
            "  \"PORCENTAJE (3°)\": {\n",
            "    \"before_nonnull\": 4,\n",
            "    \"after_numeric\": 4\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué haremos y por qué?\n",
        "\n",
        "Crear un respaldo del dataset actual sin modificar (backup CSV) para preservar la fuente antes de cualquier cambio.\n",
        "\n",
        "Reemplazar cada columna original por su columna correspondiente con sufijo _clean (si existe) para aplicar la limpieza validada en la sección anterior. Mantendremos también las columnas _clean en el archivo final para trazabilidad.\n",
        "\n",
        "Guardar el dataset resultante como data/dataset_minero_clean_applied.csv y actualizar dataset_metadata.json con el registro de cambios (qué columnas se reemplazaron, conteos antes/después, timestamp y ruta del backup).\n",
        "\n",
        "### Beneficios y trazabilidad\n",
        "\n",
        "Reversibilidad: el respaldo permite restaurar el estado previo si detectás problemas más adelante.\n",
        "\n",
        "Auditoría: metadata contiene package id/title, resource_url, columnas reemplazadas y contadores, lo que facilita reproducir y auditar exactamente lo que se cambió.\n",
        "\n",
        "Seguridad para el revisor: la limpieza ya fue aplicada solo después de la inspección en Sección 3; aquí se realiza la operación irreversible controlada y documentada."
      ],
      "metadata": {
        "id": "bhSTseq0Iv5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sección 4.1 Renombrar *_clean -> sobrescribir columnas originales con backup\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "RAW_FN = DATA_DIR / \"dataset_minero.xlsx\"\n",
        "BACKUP_RAW_CSV = DATA_DIR / \"dataset_minero_raw_backup.csv\"\n",
        "OUT_APPLIED = DATA_DIR / \"dataset_minero_clean_applied.csv\"\n",
        "META_F = DATA_DIR / \"dataset_metadata.json\"\n",
        "\n",
        "# 4.1.0) cargar df (si no existe en memoria lo leemos del clean intermedio o del raw)\n",
        "if 'df' not in globals():\n",
        "    # preferimos el clean intermedio si existe\n",
        "    candidate_clean = DATA_DIR / \"dataset_minero_clean.csv\"\n",
        "    if candidate_clean.exists():\n",
        "        df = pd.read_csv(candidate_clean, low_memory=False, encoding=\"utf-8\")\n",
        "    elif RAW_FN.exists():\n",
        "        if RAW_FN.suffix.lower() == \".csv\":\n",
        "            df = pd.read_csv(RAW_FN, low_memory=False, encoding=\"utf-8\")\n",
        "        else:\n",
        "            df = pd.read_excel(RAW_FN, engine=\"openpyxl\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No se encontró ningún dataset intermedio ni raw para aplicar los cambios.\")"
      ],
      "metadata": {
        "id": "ZLZUMC3IJGRc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1.1) identificar columnas *_clean y sus originales\n",
        "clean_cols = [c for c in df.columns if isinstance(c, str) and c.endswith(\"_clean\")]\n",
        "mapping = {}\n",
        "for c in clean_cols:\n",
        "    orig = c[:-6]  # quitar sufijo \"_clean\"\n",
        "    if orig in df.columns:\n",
        "        mapping[orig] = c\n",
        "\n",
        "if not mapping:\n",
        "    raise RuntimeError(\"No se encontraron columnas *_clean que correspondan a columnas originales. Nada que aplicar.\")\n",
        "\n",
        "print(\"Columnas a reemplazar (original -> *_clean):\")\n",
        "for orig, clean in mapping.items():\n",
        "    print(f\" - {orig}  <-  {clean}\")\n",
        "\n",
        "# 4.1.2) crear backup CSV del estado actual (antes de aplicar)\n",
        "df.to_csv(BACKUP_RAW_CSV, index=False, encoding=\"utf-8\")\n",
        "print(\"Backup creado en:\", BACKUP_RAW_CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXTPfAlVJLjH",
        "outputId": "b1d17e0d-6999-4542-b469-dc263c9c59e1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas a reemplazar (original -> *_clean):\n",
            " - PORCENTAJE (1°)  <-  PORCENTAJE (1°)_clean\n",
            " - PORCENTAJE (2°)  <-  PORCENTAJE (2°)_clean\n",
            " - PORCENTAJE (3°)  <-  PORCENTAJE (3°)_clean\n",
            "Backup creado en: data/dataset_minero_raw_backup.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1.3) aplicar reemplazo: para cada mapping, mover valores de columna_clean a columna original\n",
        "replaced = {}\n",
        "for orig, clean in mapping.items():\n",
        "    before_nonnull = int(df[orig].notna().sum()) if orig in df.columns else 0\n",
        "    # asignar valores limpios sobre la columna original\n",
        "    df[orig] = df[clean]\n",
        "    after_nonnull = int(df[orig].notna().sum())\n",
        "    replaced[orig] = {\"clean_column\": clean, \"before_nonnull\": before_nonnull, \"after_nonnull\": after_nonnull}\n",
        "\n",
        "# 4.1.4) guardar dataset final aplicado\n",
        "df.to_csv(OUT_APPLIED, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset final guardado en:\", OUT_APPLIED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzLWQ2ljJVOd",
        "outputId": "9b16585a-85b5-41d4-d65e-cfdee8817b50"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset final guardado en: data/dataset_minero_clean_applied.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1.5) actualizar metadata (leer la previa si existe y extenderla)\n",
        "meta = {}\n",
        "if META_F.exists():\n",
        "    try:\n",
        "        with open(META_F, \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "    except Exception:\n",
        "        meta = {}\n",
        "\n",
        "meta_update = {\n",
        "    \"applied_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\",\"Z\"),\n",
        "    \"backup_raw_csv\": str(BACKUP_RAW_CSV),\n",
        "    \"applied_file\": str(OUT_APPLIED),\n",
        "    \"columns_replaced\": replaced\n",
        "}\n",
        "meta.update(meta_update)\n",
        "\n",
        "with open(META_F, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nMetadata actualizada en:\", META_F)\n",
        "print(\"Resumen cambios:\")\n",
        "import pprint\n",
        "pprint.pprint(replaced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87u6FxOZJWVP",
        "outputId": "ba273e67-c253-4f82-c983-e7216dfe4f31"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata actualizada en: data/dataset_metadata.json\n",
            "Resumen cambios:\n",
            "{'PORCENTAJE (1°)': {'after_nonnull': 203,\n",
            "                     'before_nonnull': 203,\n",
            "                     'clean_column': 'PORCENTAJE (1°)_clean'},\n",
            " 'PORCENTAJE (2°)': {'after_nonnull': 30,\n",
            "                     'before_nonnull': 30,\n",
            "                     'clean_column': 'PORCENTAJE (2°)_clean'},\n",
            " 'PORCENTAJE (3°)': {'after_nonnull': 4,\n",
            "                     'before_nonnull': 4,\n",
            "                     'clean_column': 'PORCENTAJE (3°)_clean'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Features y Preprocesado reproducible\n",
        "\n",
        "## Objetivo:\n",
        "definir y aplicar transformaciones necesarias de forma reproducible y versionable. Cómo usar: ejecutar la celda de definición de funciones y pipeline, luego pipeline, maps = build_preprocessing_pipeline() y df_transformed = apply_transformations(df, pipeline, maps).\n",
        "\n",
        "Transformaciones propuestas y justificación (priorizar variables de alto impacto):\n",
        "\n",
        "## LIMPIEZA:\n",
        "Eliminar columnas vacías y normalizar nombres de columna para evitar errores de parsing en producción.\n",
        "\n",
        "## CODIFICACIÓN ORDINAL de ESTADO → ESTADO_ORD:\n",
        "Mantiene orden implícito de avance (p. ej. Prospección < Exploración < Desarrollo < Producción) y ayuda a modelos lineales/árbol.\n",
        "\n",
        "## AGRUPACIÓN REGIONAL (PROVINCIA → REGION):\n",
        "reduce cardinalidad y captura correlaciones geográficas.\n",
        "\n",
        "## ENCODING MINERAL (MINERAL PRINCIPAL):\n",
        "target/one-hot según modelo; por defecto usamos OneHotEncoder limitado a top-k (resto -> Otros).\n",
        "\n",
        "## LAT/LONG:\n",
        "mantener como numérico y generar columnas de lat-long normalizadas (StandardScaler). Posible derivación: cluster geográfico (KMeans) opcional.\n",
        "\n",
        "## PORCENTAJES:\n",
        "usar columnas ya normalizadas (_clean) y rellenar NaN con 0 si semántica indica ausencia.\n",
        "\n",
        "## FEATURES DERIVADOS:\n",
        "Contar controlantes (si hay múltiples columnas CONTROLANTE n°) -> número de empresas asociadas; texto a bandera (presence/absence).\n",
        "\n",
        "## TIPOS Y CONSISTENCIA:\n",
        "Asegurar tipos numéricos para todas las columnas numéricas y strings para categóricas.\n",
        "\n",
        "## Checks mínimos:\n",
        "No pérdida de filas al aplicar pipeline.\n",
        "Tipos finales correctos.\n",
        "Reutilizabilidad: apply_transformations devuelve df y diccionarios de mapeo/encoders.\n",
        "\n",
        "Artefactos:\n",
        "pipeline.joblib (pipeline completo)\n",
        "encoders.joblib (mapas simples, p. ej. ESTADO map)\n",
        "pipeline_description.md (lista de transformaciones aplicadas y orden)"
      ],
      "metadata": {
        "id": "0LLd_x-6LG1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers y mapeos\n",
        "\n",
        "Normaliza porcentajes de forma consistente y crea columnas *_clean sin sobrescribir lo original.\n",
        "\n",
        "Genera un snapshot y metadata mínima para trazabilidad sin saturar la salida."
      ],
      "metadata": {
        "id": "m9s3Ki3qUiBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.0 Imporactión\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 5\n",
        "DATA_DIR = Path(\"data\")\n",
        "ART = DATA_DIR / \"artifacts\"\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CLEAN_APPLIED = DATA_DIR / \"dataset_minero_clean_applied.csv\"\n",
        "RAW_FN = DATA_DIR / \"dataset_minero.xlsx\"\n",
        "if CLEAN_APPLIED.exists():\n",
        "    df = pd.read_csv(CLEAN_APPLIED, low_memory=False)\n",
        "elif RAW_FN.exists():\n",
        "    df = pd.read_excel(RAW_FN, engine=\"openpyxl\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No se encontró dataset_minero_clean_applied.csv ni dataset_minero.xlsx en data/\")\n",
        "\n",
        "# Resumen mínimo para revisión rápida\n",
        "print(\"Carga OK — filas:\", len(df), \"columnas:\", len(df.columns))\n",
        "print(\"Principales dtypes:\\n\", df.dtypes.value_counts().to_dict())\n",
        "\n",
        "# Helper compacto para normalizar PORCENTAJE simples (usa heurística leve)\n",
        "def to_numeric_pct(col):\n",
        "    s = (col.astype(str)\n",
        "         .str.replace(r\"[^\\d,.\\-%]\", \"\", regex=True)\n",
        "         .str.replace(\"%\", \"\", regex=False)\n",
        "         .str.replace(\",\", \".\", regex=False))\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "# Aplicar sólo a columnas que contienen 'PORCENTAJE' en el nombre\n",
        "pct_cols = [c for c in df.columns if \"PORCENTAJE\" in c.upper()]\n",
        "for c in pct_cols:\n",
        "    df[f\"{c}_clean\"] = to_numeric_pct(df[c])\n",
        "\n",
        "# Guardar snapshot ligero y metadata mínima\n",
        "(df.head(200)\n",
        "   .to_csv(DATA_DIR / \"data_snapshot_head_clean.csv\", index=False))\n",
        "meta = {\"loaded_at\": pd.Timestamp.now().isoformat(), \"rows\": len(df), \"cols\": list(df.columns)[:50]}\n",
        "(ART / \"preproc_light_meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Columnas PORCENTAJE (detectadas):\", pct_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEHV5QbgKsfS",
        "outputId": "923ddde4-cad3-4c73-97bd-055a909725d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carga OK — filas: 325 columnas: 20\n",
            "Principales dtypes:\n",
            " {dtype('O'): 11, dtype('float64'): 8, dtype('int64'): 1}\n",
            "Columnas PORCENTAJE (detectadas): ['PORCENTAJE (1°)', 'PORCENTAJE (2°)', 'PORCENTAJE (3°)', 'PORCENTAJE (1°)_clean', 'PORCENTAJE (2°)_clean', 'PORCENTAJE (3°)_clean']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detección de columnas numéricas candidatas y creación segura de columnas *_clean\n",
        "\n",
        "\n",
        "*   identifica columnas numéricas candidatas (incluye heurística para columnas con \"PORCENTAJE\" o nombres típicos),\n",
        "\n",
        "* crea columnas *_clean solo si faltan y cuando la conversión aporta valores numéricos,\n",
        "\n",
        "* valida que haya al menos una columna numérica para el pipeline; falla con mensaje claro si no las hay,\n",
        "\n",
        "* construye un ColumnTransformer compacto (numérico + categórico básico),\n",
        "\n",
        "* realiza un fit/transform de comprobación sobre un subconjunto (head) y guarda artefactos mínimos (lista de columnas y pipeline provisoria). de lista\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xt9RGfNCLJZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.A detectar numéricos, crear *_clean mínimos y construir preprocesamiento\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# heurística ligera para convertir porcentajes y textos numéricos a float\n",
        "def to_numeric_simple(s):\n",
        "    s = pd.Series(s).astype(str).str.replace(r\"[^\\d\\-,.\\%]\", \"\", regex=True).str.replace(\"%\",\"\",regex=False).str.replace(\",\",\".\")\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "# detectar columnas candidatas\n",
        "cols = df.columns.tolist()\n",
        "pct_cols = [c for c in cols if \"PORCENTAJE\" in c.upper()]\n",
        "numeric_by_dtype = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "name_based = [c for c in cols if any(k in c.upper() for k in (\"LATITUD\",\"LONGITUD\",\"Nº\",\"NUM\",\"KG\",\"TON\",\"PORC\"))]\n",
        "candidates = list(dict.fromkeys(name_based + pct_cols + numeric_by_dtype))\n",
        "\n",
        "# crear *_clean solo si falta y la conversión aporta valores numéricos\n",
        "created = []\n",
        "for c in candidates:\n",
        "    clean_name = f\"{c}_clean\" if not c.endswith(\"_clean\") else c\n",
        "    if clean_name in df.columns:\n",
        "        continue\n",
        "    if c in pct_cols or not pd.api.types.is_numeric_dtype(df[c]):\n",
        "        num = to_numeric_simple(df[c])\n",
        "        if num.notna().sum() >= 1:\n",
        "            df[clean_name] = num\n",
        "            created.append(clean_name)\n",
        "\n",
        "# elegir columnas numéricas preferiendo *_clean\n",
        "numeric_cols = []\n",
        "for c in candidates:\n",
        "    cand = f\"{c}_clean\" if f\"{c}_clean\" in df.columns else c\n",
        "    if pd.api.types.is_numeric_dtype(df[cand]) or df[cand].notna().sum()>0:\n",
        "        numeric_cols.append(cand)\n",
        "numeric_cols = [c for c in numeric_cols if c.lower() != \"target\"]\n",
        "\n",
        "if len(numeric_cols) == 0:\n",
        "    raise ValueError(\"No se detectaron columnas numéricas válidas para el pipeline. Revisa heurística o crea *_clean.\")\n",
        "\n",
        "# categóricas de baja cardinalidad\n",
        "cat_cols = [c for c in df.columns if pd.api.types.is_object_dtype(df[c]) and df[c].nunique(dropna=True) <= 50][:20]\n",
        "\n",
        "# pipeline compacto\n",
        "num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
        "cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")), (\"ohe\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))])\n",
        "preproc = ColumnTransformer([(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, cat_cols)], remainder=\"drop\", verbose_feature_names_out=False)\n",
        "\n",
        "# fit/transform de comprobación rápida (head)\n",
        "X_check = df.drop(columns=[c for c in df.columns if c.lower()==\"target\"]) if \"target\" in df.columns else df.copy()\n",
        "preproc.fit(X_check.head(200))\n",
        "Xt = preproc.transform(X_check.head(200))\n",
        "\n",
        "# persistir artefactos mínimos\n",
        "joblib.dump(preproc, ART/\"preproc_compact.joblib\")\n",
        "(ART/\"preproc_columns.json\").write_text(json.dumps({\"numeric_cols\": numeric_cols, \"cat_cols\": cat_cols, \"created_clean\": created}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Preproc compacto creado. Numeric cols:\", numeric_cols[:10], \"Cat cols:\", cat_cols[:6], \"Created:\", created)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSMC2iWuLpYj",
        "outputId": "dab1ad2a-c556-4d71-c79d-840eef09cf93"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preproc compacto creado. Numeric cols: ['LATITUD', 'LONGITUD', 'PORCENTAJE (1°)_clean', 'PORCENTAJE (2°)_clean', 'PORCENTAJE (3°)_clean', 'PORCENTAJE (1°)_clean_clean', 'PORCENTAJE (2°)_clean_clean', 'PORCENTAJE (3°)_clean_clean', 'PORCENTAJE (1°)_clean_clean', 'PORCENTAJE (2°)_clean_clean'] Cat cols: ['MINERAL PRINCIPAL', 'PROVINCIA', 'ESTADO', 'ORIGEN (1°)', 'CONTROLANTE (2°)', 'ORIGEN (2°)'] Created: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### builder del pipeline\n",
        "\n",
        "Extrae funciones puras para mappings (ESTADO ordinal, provincia → región, top-k minerales) y las persiste en un mapa simple.\n",
        "\n",
        "Valida tempranamente listas de columnas (numeric_cols no vacía) y falla con mensaje claro si no hay candidatos válidos.\n",
        "\n",
        "Fitea sólo sobre X (features) y deja fuera la columna target; devuelve (pipeline_fitted, maps) y escribe artefactos en data/artifacts para trazabilidad."
      ],
      "metadata": {
        "id": "iNNhgQ4MUnCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.B Refactor: build_preprocessing_pipeline compacto, seguro y serializable\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Helpers reutilizables\n",
        "def make_clean_name(c):\n",
        "    return c if str(c).endswith(\"_clean\") else f\"{c}_clean\"\n",
        "\n",
        "def topk_values(series, k=8):\n",
        "    return series.fillna(\"Desconocido\").astype(str).value_counts().index[:k].tolist()\n",
        "\n",
        "# Cargar df (aplicado)\n",
        "APPLIED = DATA_DIR / \"dataset_minero_clean_applied.csv\"\n",
        "if APPLIED.exists():\n",
        "    df = pd.read_csv(APPLIED, low_memory=False)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Ejecuta la limpieza y aplica los *_clean antes de construir el pipeline.\")\n",
        "\n",
        "# Detectar columnas candidatas\n",
        "cols = df.columns.tolist()\n",
        "pct_like = [c for c in cols if \"PORCENTAJE\" in str(c).upper()]\n",
        "dtype_num = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "name_num = [c for c in cols if any(k in str(c).upper() for k in (\"LATITUD\",\"LONGITUD\",\"N°\",\"Nº\",\"NUM\",\"PORC\"))]\n",
        "cand = list(dict.fromkeys(name_num + pct_like + dtype_num))\n",
        "\n",
        "# Preferir *_clean si existe, evitando duplicados\n",
        "final_num = []\n",
        "for c in cand:\n",
        "    clean_c = f\"{c}_clean\" if f\"{c}_clean\" in df.columns else c\n",
        "    if (pd.api.types.is_numeric_dtype(df[clean_c]) or df[clean_c].notna().sum()>0) and clean_c not in final_num:\n",
        "        final_num.append(clean_c)\n",
        "\n",
        "if not final_num:\n",
        "    raise ValueError(\"No numeric columns detected for numeric pipeline. Revisa *_clean o pasa numeric_candidates explícito.\")\n",
        "\n",
        "# Categóricas de baja cardinalidad\n",
        "cat_candidates = [c for c in df.columns if pd.api.types.is_object_dtype(df[c]) or isinstance(df[c].dtype, pd.CategoricalDtype)]\n",
        "cat_cols = [c for c in cat_candidates if df[c].nunique(dropna=True) <= 50][:20]\n",
        "\n",
        "# Define pipes\n",
        "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
        "estado_pipe = Pipeline([(\"estado_ord\", None), (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1))])  # placeholder si no existe EstadoOrdinalTransformer\n",
        "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")), (\"ohe\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))])\n",
        "\n",
        "# Build transformers list solo con columnas no vacías\n",
        "transformers = []\n",
        "if final_num:\n",
        "    transformers.append((\"num\", num_pipe, final_num))\n",
        "if \"ESTADO\" in df.columns:\n",
        "    transformers.append((\"estado\", estado_pipe, [\"ESTADO\"]))\n",
        "if \"PROVINCIA\" in df.columns:\n",
        "    transformers.append((\"region\", Pipeline([(\"region_map\", None), (\"ohe\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))]), [\"PROVINCIA\"]))\n",
        "if \"MINERAL PRINCIPAL\" in df.columns:\n",
        "    transformers.append((\"mineral\", Pipeline([(\"topk\", None), (\"ohe\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))]), [\"MINERAL PRINCIPAL\"]))\n",
        "if cat_cols:\n",
        "    transformers.append((\"cat\", cat_pipe, cat_cols))\n",
        "\n",
        "transformers = [(n,t,cols) for (n,t,cols) in transformers if cols]\n",
        "if not transformers:\n",
        "    raise RuntimeError(\"No transformers disponibles para ColumnTransformer. Revisa columnas candidatas.\")\n",
        "\n",
        "ct = ColumnTransformer(transformers, remainder=\"drop\", verbose_feature_names_out=False, n_jobs=1)\n",
        "pipeline = Pipeline([(\"preproc\", ct)])"
      ],
      "metadata": {
        "id": "G4yy4rjmMeWW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fit, transform, reconstrucción mínima y test"
      ],
      "metadata": {
        "id": "hWwQCKMUUuT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4.C Fit preproc, transformar df\n",
        "# -------------------------\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "possible_targets = [c for c in df.columns if c.lower() in (\"objetivo\",\"target\",\"label\",\"y\")]\n",
        "target_col = possible_targets[0] if possible_targets else None\n",
        "X = df.drop(columns=[target_col]) if target_col else df.copy()\n",
        "pipeline.fit(X)\n",
        "\n",
        "# Transform completo y persistencia\n",
        "Xt = pipeline.transform(X)\n",
        "try:\n",
        "    feature_names = pipeline.named_steps[\"preproc\"].get_feature_names_out()\n",
        "except Exception:\n",
        "    feature_names = [f\"f{i}\" for i in range(Xt.shape[1])]\n",
        "\n",
        "def sanitize(cols):\n",
        "    return [str(c).strip().replace(\" \", \"_\").replace(\",\", \"\").replace(\"(\",\"\").replace(\")\",\"\") for c in cols]\n",
        "\n",
        "feature_names = sanitize(feature_names)\n",
        "df_trans = pd.DataFrame(Xt, columns=feature_names, index=X.index)\n",
        "if target_col:\n",
        "    df_trans[target_col] = df[target_col].values\n",
        "\n",
        "# Persistir artefactos\n",
        "joblib.dump(pipeline, ART / \"preproc_fitted.joblib\")\n",
        "(ART / \"preproc_maps.json\").write_text(json.dumps({\"numeric_cols\": final_num, \"cat_cols\": cat_cols}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "df_trans.to_parquet(ART / \"df_transformed.parquet\", index=True)\n",
        "(df_trans.head(5).to_csv(ART / \"df_trans_head5.csv\", index=True))\n",
        "\n",
        "print(\"Preproc fitted and saved to:\", ART / \"preproc_fitted.joblib\")\n",
        "print(\"df_trans persisted to:\", ART / \"df_transformed.parquet\")\n",
        "print(\"Transformed shape:\", df_trans.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl1DOJ8hMxPX",
        "outputId": "537d80bf-8cea-46de-f405-b865ee9e8415"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preproc fitted and saved to: data/artifacts/preproc_fitted.joblib\n",
            "df_trans persisted to: data/artifacts/df_transformed.parquet\n",
            "Transformed shape: (325, 136)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revisión, Checks sobre transformaciones\n",
        "\n",
        "Objetivos de la celda\n",
        "Verificar integridad de df_trans (filas, índice consistente, target no filtrado ni duplicado).\n",
        "\n",
        "Comprobar tipos y recomendaciones: columnas object residuales, columnas numéricas con alta proporción de NaN, columnas constantes.\n",
        "\n",
        "Detectar riesgo de data leakage (target presente entre features).\n",
        "\n",
        "Emitir resumen compacto en notebook y registrar un log detallado en data/artifacts/preproc_review.json para auditoría.\n",
        "\n",
        "Devolver un status simple (\"OK\" / \"WARN\" / \"ERROR\") para gates posteriores del notebook."
      ],
      "metadata": {
        "id": "O7shFaDSNVlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.D Revisar: checks finales sobre df_trans y preproc_fitted\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "df_trans_path = ART / \"df_transformed.parquet\"\n",
        "pipeline_path = ART / \"preproc_fitted.joblib\"\n",
        "\n",
        "report = {\"checked_at\": pd.Timestamp.now().isoformat()}\n",
        "report[\"files_present\"] = {\"df_transformed\": df_trans_path.exists(), \"preproc_fitted\": pipeline_path.exists()}\n",
        "\n",
        "if df_trans_path.exists():\n",
        "    df_trans = pd.read_parquet(df_trans_path)\n",
        "    report[\"original_rows\"] = int(df_trans.shape[0])\n",
        "    report[\"original_cols\"] = int(df_trans.shape[1])\n",
        "    report[\"index_unique\"] = bool(df_trans.index.is_unique)\n",
        "    report[\"n_duplicates\"] = int(df_trans.duplicated().sum())\n",
        "    possible_targets = [c for c in df_trans.columns if c.lower() in (\"target\",\"label\",\"objetivo\",\"y\")]\n",
        "    report[\"present_targets_detected\"] = possible_targets\n",
        "    report[\"dtypes_summary\"] = df_trans.dtypes.astype(str).value_counts().to_dict()\n",
        "    report[\"high_nan_columns\"] = [c for c,v in (df_trans.isna().mean()).items() if v>0.5][:20]\n",
        "    report[\"constant_columns\"] = [c for c in df_trans.columns if df_trans[c].nunique(dropna=False) <= 1][:20]\n",
        "else:\n",
        "    report[\"error\"] = \"df_transformed.parquet not found; run preprocessing fit/transform first.\"\n",
        "\n",
        "(ART / \"preproc_review.json\").write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"4.D Revisar — estado:\", \"OK\" if \"error\" not in report else \"ERROR\")\n",
        "print(\"Detailed report saved to:\", ART / \"preproc_review.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAK8AXCVNKIe",
        "outputId": "e0307c90-5331-4375-9c30-616d4f84c465"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.D Revisar — estado: OK\n",
            "Detailed report saved to: data/artifacts/preproc_review.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limpieza final y persistencia de features\n",
        "\n",
        "Eliminar columnas constantes y casi-constantes.\n",
        "\n",
        "Eliminar columnas con alto porcentaje de NaNs (umbral razonable 50%).\n",
        "\n",
        "Opcional: eliminar columnas colineales (corr>0.95) o aplicar Threshold de varianza baja.\n",
        "\n",
        "Persistir lista final de features y df_final (sin target separado) para modelado reproducible."
      ],
      "metadata": {
        "id": "5km_W_5ZN5cx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07926973"
      },
      "source": [
        "## Conclusiones sobre Features, Preprocesado y Limpieza Final\n",
        "\n",
        "La fase de 'Features y Preprocesado' ha sido un pilar fundamental en la construcción de un dataset robusto y adecuado para el modelado predictivo. A lo largo de este proceso, se lograron los siguientes hitos clave:\n",
        "\n",
        "1.  **Consolidación del Dataset (`df_model_ready`):** Se partió de un dataset inicial que pasó por múltiples etapas de limpieza y transformación. El resultado final es `df_model_ready`, un `DataFrame` estandarizado, libre de valores problemáticos y con las características ingenierizadas, listo para alimentar los modelos.\n",
        "\n",
        "2.  **Limpieza Profunda y Controlada:**\n",
        "    *   **Manejo de Columnas de Porcentaje:** Se implementó una lógica rigurosa para convertir las columnas de porcentaje a formato numérico (`float`), gestionando caracteres no numéricos y detectando la escala (0-100% vs 0-1). Las columnas redundantes (`_clean`) fueron eliminadas explícitamente para mantener la claridad.\n",
        "    *   **Identificación y Eliminación de Residuos:** Se eliminaron columnas completamente vacías o irrelevantes, como `Unnamed: 16`, que a menudo aparecen como artefactos de la carga de datos.\n",
        "    *   **Saneamiento de Valores Infinitos y `NaN`s:** Se integraron pasos robustos para reemplazar valores `inf` por `NaN`, y para imputar `NaN`s utilizando estrategias consistentes (valor constante 0 o mediana) para garantizar que el dataset numérico sea compatible con los algoritmos de ML.\n",
        "\n",
        "3.  **Ingeniería de Features Orientada al Dominio:**\n",
        "    *   **`ESTADO_ORD_FEATURE`:** Se transformó la variable `ESTADO` en una escala ordinal (`0` a `4`), capturando la progresión natural de los proyectos mineros, lo que permite a los modelos interpretar la secuencia de avance.\n",
        "    *   **`REGION_FEATURE`:** Se agrupó la `PROVINCIA` en categorías regionales (`Norte`, `Centro`, `Sur`, `Otra`, `Desconocido`), reduciendo la cardinalidad y permitiendo al modelo aprender patrones geográficos más generales.\n",
        "    *   **`MINERAL_AGRUPADO`:** La variable `MINERAL PRINCIPAL` fue procesada con 'one-hot encoding' después de agrupar los minerales menos frecuentes en una categoría 'Otros', optimizando la dimensionalidad y manteniendo la capacidad predictiva.\n",
        "\n",
        "4.  **Pipeline de Preprocesamiento Robusto y Reproducible:**\n",
        "    *   Se utilizó `sklearn.pipeline.Pipeline` y `sklearn.compose.ColumnTransformer` para encapsular todas las transformaciones de manera modular. Esto asegura que los pasos de limpieza y feature engineering se apliquen de forma consistente tanto en el entrenamiento como en la inferencia.\n",
        "    *   Se resolvieron desafíos técnicos significativos, como el manejo de errores `ValueError: Found array with 0 sample(s)` en el `SimpleImputer` (mediante el filtrado de columnas demasiado dispersas y la estrategia de imputación `constant=0`), y problemas de compatibilidad de nombres de características con el `ColumnTransformer`.\n",
        "    *   La configuración de `n_jobs=1` en el `ColumnTransformer` y la sanitización explícita a `float32` garantizaron la estabilidad y compatibilidad con las librerías de modelado.\n",
        "\n",
        "5.  **Optimización y Reducción de Dimensionalidad Final:**\n",
        "    *   Se realizó una fase de limpieza final en `df_model_ready` para eliminar columnas con varianza cero (constantes), columnas con un porcentaje excesivo de `NaN`s (aunque el preprocesamiento ya las manejó) y columnas altamente colineales. Esto resultó en un `DataFrame` más eficiente y menos propenso al overfitting.\n",
        "    *   La lista final de características utilizada para el modelado se persiste en `data/artifacts/features_final.json`, garantizando la trazabilidad y la reproducibilidad.\n",
        "\n",
        "El `df_model_ready` resultante, con **325 filas y 96 columnas**, es la culminación de un proceso de preparación de datos meticuloso. Este dataset está ahora completamente listo para la fase de modelado, permitiendo que los algoritmos de Machine Learning se enfoquen en aprender patrones predictivos sobre la etapa avanzada de los proyectos mineros."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Estrategia de split y validación reproducible\n",
        "\n",
        "## Objetivo\n",
        "Definir y persistir un split reproducible (train / val / test) y, opcionalmente, folds de CV estratificados. Justificación: reservar un test final fijo (10–20%) para estimación no sesgada del rendimiento; usar validación cruzada estratificada en el entrenamiento para selección de modelos cuando hay clase desbalanceada o variables categóricas con distinta prevalencia. Guardamos índices y un resumen para auditoría y reproducción exacta.\n",
        "\n",
        "## Decisiones prácticas que tomamos para este analisis\n",
        "\n",
        "Test final: 15% por defecto (ajustable).\n",
        "\n",
        "Val: 15% (o usar CV dentro del train si prefieres k-fold).\n",
        "\n",
        "Estratificar por la variable target si es categórica; si no existe una columna target obvia, el código intenta inferir varias opciones razonables y falla con mensaje claro.\n",
        "\n",
        "Semilla fija (RANDOM_STATE) para reproducibilidad.\n",
        "\n",
        "Guardar índices en joblib y un CSV resumen con distribuciones por split."
      ],
      "metadata": {
        "id": "uTYT9EaFZnl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estrategia elegida: train / val / test con test final retenido para evaluación definitiva.\n",
        "#### Proporciones por defecto: train 70%, val 15%, test 15% (modificable).\n",
        "#### Estratificación: se aplica si la variable target es categórica; si no, se realiza un split aleatorio mantenido por semilla.\n",
        "#### Reproducibilidad: indices guardados en joblib; resume de distribuciones guardado en split_summary.csv.\n",
        "\n",
        "\n",
        "**Uso: ejecutar la celda de código siguiente para crear splits; luego usar indices guardados para entrenar/evaluar modelos sin re-samplear.**\n"
      ],
      "metadata": {
        "id": "10dnNWLQZ6X0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gbyYhSsyN-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.A Crear splits reproducibles (train/val/test) y guardar índices\n",
        "import os, joblib, json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "ART = DATA_DIR / \"artifacts\"\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.15   # ajustar si preferís 0.1 o 0.2\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "# --- MODIFICACIÓN CLAVE: Asegurar el target binario para los splits ---\n",
        "# Asegurarse de que df_trans esté disponible globalmente\n",
        "if 'df_trans' not in globals():\n",
        "    raise RuntimeError(\"df_trans no está disponible. Ejecute las celdas de preprocesamiento y transformación primero.\")\n",
        "\n",
        "# Definir la variable objetivo binaria 'etapa_avanzada'\n",
        "etapas_avanzadas = {\n",
        "    \"Producción\", \"Desarrollo\", \"Factibilidad\", \"Exploración avanzada\",\n",
        "    \"Construcción\", \"Prefactibilidad\", \"Evaluación Económica Preliminar\"\n",
        "}\n",
        "\n",
        "# Obtener la columna original 'ESTADO' del DataFrame `df` (el raw o clean_applied)\n",
        "# Asumimos que `df` (el original cargado en gRhFwdb2TsWS) contiene la columna 'ESTADO'\n",
        "if 'df' not in globals() or 'ESTADO' not in df.columns:\n",
        "    # df debería estar cargado y procesado por gRhFwdb2TsWS\n",
        "    processed_df_path = Path(\"data\") / \"dataset_minero_clean_applied.csv\"\n",
        "    if not processed_df_path.exists():\n",
        "        raise FileNotFoundError(f\"Processed dataframe not found at {processed_df_path}. Please run previous cleaning steps.\")\n",
        "    df_original_estado = pd.read_csv(processed_df_path, low_memory=False)\n",
        "else:\n",
        "    df_original_estado = df # Si df ya está en memoria y es el correcto\n",
        "\n",
        "if 'ESTADO' not in df_original_estado.columns:\n",
        "    raise RuntimeError(\"La columna 'ESTADO' no está disponible en el DataFrame. Asegúrese de que el preprocesamiento la mantenga.\")\n",
        "\n",
        "# Crear la columna 'etapa_avanzada' en un DataFrame auxiliar para los splits\n",
        "# Usamos `df_trans.index` para asegurar que el mapeo sea correcto\n",
        "df_for_split = df_trans.copy() # Usamos df_trans que ya tiene los índices consistentes con el resto de X\n",
        "df_for_split['etapa_avanzada'] = df_original_estado['ESTADO'].apply(lambda x: 1 if x in etapas_avanzadas else 0)\n",
        "\n",
        "# Definir la columna target para los splits\n",
        "target_col = 'etapa_avanzada'\n",
        "\n",
        "# Asegurar no usar filas con NA en target (aunque y_binary no debería tenerlos si df_original_estado['ESTADO'] no tiene NaNs)\n",
        "mask_valid = ~df_for_split[target_col].isna()\n",
        "if mask_valid.sum() != len(df_for_split):\n",
        "    print(f\"Aviso: se excluyen {len(df_for_split)-mask_valid.sum()} filas con target NA del split.\")\n",
        "df_split = df_for_split[mask_valid].reset_index(drop=True)\n",
        "# --- FIN DE LA MODIFICACIÓN CLAVE ---\n",
        "\n",
        "\n",
        "# Verificar si la estratificación es posible para el target seleccionado\n",
        "class_counts = df_split[target_col].value_counts()\n",
        "min_class_count = class_counts.min()\n",
        "\n",
        "is_stratify_overall = False\n",
        "# Ahora el target_col es numérico (0 o 1), pero lo tratamos como categórico para estratificar\n",
        "if df_split[target_col].nunique() == 2: # Si es binario\n",
        "    if min_class_count >= 2: # Necesita al menos 2 para el split principal\n",
        "        is_stratify_overall = True\n",
        "    else:\n",
        "        print(f\"Advertencia: No es posible estratificar por '{target_col}' en el split principal porque al menos una clase tiene solo {min_class_count} miembro(s).\")\n",
        "        print(\"Conteo de clases en el target:\", class_counts.to_string())\n",
        "else:\n",
        "     print(f\"Advertencia: El target '{target_col}' no es binario. La estratificación se intentará si es multiclase con suficientes muestras.\")\n",
        "     if min_class_count >= 2: # Si es multiclase, también necesita al menos 2\n",
        "        is_stratify_overall = True\n",
        "     else:\n",
        "        print(f\"Advertencia: No es posible estratificar por '{target_col}' en el split principal porque al menos una clase tiene solo {min_class_count} miembro(s).\")\n",
        "        print(\"Conteo de clases en el target:\", class_counts.to_string())\n",
        "\n",
        "\n",
        "# Primer split: train+val vs test\n",
        "if is_stratify_overall:\n",
        "    trainval_idx, test_idx = train_test_split(df_split.index.to_numpy(), test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_split[target_col])\n",
        "else:\n",
        "    trainval_idx, test_idx = train_test_split(df_split.index.to_numpy(), test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "# Segundo split: train vs val desde trainval (proporción relativa)\n",
        "rel_val_size = VAL_SIZE / (1.0 - TEST_SIZE)  # fracción de trainval que irá a val\n",
        "\n",
        "is_stratify_trainval = False\n",
        "if is_stratify_overall: # Solo intentamos estratificar train/val si el split general fue estratificable\n",
        "    trainval_class_counts = df_split.loc[trainval_idx, target_col].value_counts()\n",
        "    if trainval_class_counts.min() >= 2: # Necesita al menos 2 para el split de train/val\n",
        "        is_stratify_trainval = True\n",
        "    else:\n",
        "        print(f\"Advertencia: No es posible estratificar por '{target_col}' en el split train/val porque al menos una clase tiene muy pocos miembros en el conjunto trainval.\")\n",
        "        print(\"Conteo de clases en el target para trainval:\", trainval_class_counts.to_string())\n",
        "\n",
        "if is_stratify_trainval:\n",
        "    train_idx, val_idx = train_test_split(trainval_idx, test_size=rel_val_size, random_state=RANDOM_STATE, stratify=df_split.loc[trainval_idx, target_col])\n",
        "else:\n",
        "    train_idx, val_idx = train_test_split(trainval_idx, test_size=rel_val_size, random_state=RANDOM_STATE)\n",
        "\n",
        "# Guardar índices (son índices relativos a df_split)\n",
        "indices_out = {\n",
        "    \"train_idx\": train_idx.tolist(),\n",
        "    \"val_idx\": val_idx.tolist(),\n",
        "    \"test_idx\": test_idx.tolist(),\n",
        "    \"target_col\": target_col,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"test_size\": TEST_SIZE,\n",
        "    \"val_size\": VAL_SIZE,\n",
        "    \"is_stratified_overall\": is_stratify_overall,\n",
        "    \"is_stratified_trainval\": is_stratify_trainval\n",
        "}\n",
        "joblib.dump(indices_out, ART / \"indices_train_val_test.joblib\")\n",
        "\n",
        "# Crear resumen de distribuciones por split y guardarlo\n",
        "def summarize_split(idxs, name):\n",
        "    sub = df_split.loc[idxs]\n",
        "    d = {\"n_rows\": len(sub)}\n",
        "    # si categórica, mostrar counts por category; si numérica, mostrar quantiles\n",
        "    if not pd.api.types.is_numeric_dtype(sub[target_col]) or sub[target_col].nunique() < 20:\n",
        "        d[\"value_counts\"] = sub[target_col].value_counts(dropna=False).to_dict()\n",
        "    else:\n",
        "        d[\"quantiles\"] = sub[target_col].quantile([0,0.25,0.5,0.75,1.0]).to_dict()\n",
        "    return d\n",
        "\n",
        "summary = {\n",
        "    \"train\": summarize_split(train_idx, \"train\"),\n",
        "    \"val\": summarize_split(val_idx, \"val\"),\n",
        "    \"test\": summarize_split(test_idx, \"test\"),\n",
        "    \"total_rows_available\": int(len(df_split))\n",
        "}\n",
        "# guardar resumen csv y json\n",
        "pd.DataFrame({\n",
        "    \"split\":[\"train\",\"val\",\"test\"],\n",
        "    \"n_rows\":[len(train_idx), len(val_idx), len(test_idx)]\n",
        "}).to_csv(ART / \"split_counts.csv\", index=False)\n",
        "(ART / \"split_summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Splits creados y guardados en:\", ART)\n",
        "print(\"Counts -> train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx))"
      ],
      "metadata": {
        "id": "TzAjulc1aRPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd242bcb-9488-4126-a8ec-6a85c3f51d3e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits creados y guardados en: data/artifacts\n",
            "Counts -> train: 227 val: 49 test: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "ART = Path(\"data/artifacts\")\n",
        "df_trans = pd.read_parquet(ART / \"df_transformed.parquet\")\n",
        "print(\"Shape:\", df_trans.shape)\n",
        "print(\"Columnas:\", df_trans.columns.tolist())\n",
        "print(\"Detectados posibles target:\", [c for c in df_trans.columns if c.lower() in ('target','label','objetivo','y')])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCb6P7WRyToR",
        "outputId": "94822a38-4f7a-479d-9440-0fb3cc62a72c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (325, 138)\n",
            "Columnas: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'ESTADO', 'target']\n",
            "Detectados posibles target: ['target']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "661a17b5"
      },
      "source": [
        "### La fase de split de datos se ha completado exitosamente, estableciendo conjuntos reproducibles de entrenamiento, validación y prueba para el desarrollo y evaluación del modelo.\n",
        "\n",
        "### Estrategia y Ejecución:\n",
        "1.  **Objetivo:** El objetivo principal fue crear un split reproducible de los datos, reservando un conjunto de prueba final para la evaluación no sesgada del rendimiento del modelo, y conjuntos de entrenamiento y validación para el desarrollo del modelo.\n",
        "2.  **Variable Target:** Se identificó la columna `'ESTADO'` como la variable objetivo principal para el split.\n",
        "3.  **Manejo de Stratificación:**\n",
        "    *   Se intentó realizar una estratificación por la columna `ESTADO` para asegurar una distribución similar de las clases en cada subconjunto (entrenamiento, validación, prueba).\n",
        "    *   Sin embargo, se detectó que al menos una clase (`'Cese de operaciones'`) tenía solo 1 miembro, lo cual es insuficiente para la estratificación (se necesitan al menos 2 miembros por clase para dividir). En estos casos, se procedió con un split aleatorio para ese paso, evitando un error y garantizando la continuidad del proceso.\n",
        "4.  **Proporciones del Split:** Se utilizaron las proporciones definidas de `15%` para el conjunto de prueba (`test`) y `15%` para el conjunto de validación (`val`) (relativo al resto de los datos), con el resto asignado al conjunto de entrenamiento (`train`).\n",
        "\n",
        "### Resultados Finales:\n",
        "*   **Conjunto de Entrenamiento (train):** 227 filas.\n",
        "*   **Conjunto de Validación (val):** 49 filas.\n",
        "*   **Conjunto de Prueba (test):** 49 filas.\n",
        "\n",
        "### Reproducibilidad:\n",
        "Los índices de cada subconjunto (`train_idx`, `val_idx`, `test_idx`) junto con la configuración del split (`target_col`, `random_state`, `test_size`, `val_size` y la información de si se estratificó o no) han sido guardados en `data/artifacts/indices_train_val_test.joblib`. Además, se generaron archivos `split_counts.csv` y `split_summary.json` para auditar las distribuciones y tamaños de cada split.\n",
        "\n",
        "Con estos conjuntos de datos definidos de manera reproducible, se puede proceder con confianza a la fase de modelado, sabiendo que la evaluación final se realizará sobre un conjunto de datos virgen (`test_idx`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-fold estratificado sobre train para CV y guardar folds"
      ],
      "metadata": {
        "id": "NMC3IqGxcG6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.B Generar StratifiedKFold sobre el conjunto de train (si aplica) y guardar folds\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import joblib\n",
        "\n",
        "# cargar índices previos\n",
        "meta = joblib.load(ART / \"indices_train_val_test.joblib\")\n",
        "train_idx = np.array(meta[\"train_idx\"])\n",
        "target_col = meta[\"target_col\"]\n",
        "\n",
        "\n",
        "\n",
        "# If df_split is not in globals (e.g., if notebook restarted mid-way), recreate it safely.\n",
        "if 'df_split' not in globals():\n",
        "    # This part should ideally not run if previous cells were executed correctly\n",
        "    # df_trans should be available from gRhFwdb2TsWS and c0rp5LpcjJh4 should have created y_binary and X_full\n",
        "    print(\"Warning: df_split not found. Re-deriving from X_full and y_binary.\")\n",
        "    temp_df = X_full.copy() # Use X_full as it's the fully processed feature set\n",
        "    temp_df[target_col] = y_binary # Attach the binary target (if y_binary is correctly formed)\n",
        "    # Filter out NaNs in target for split, as done in TzAjulc1aRPj\n",
        "    mask_valid = ~temp_df[target_col].isna()\n",
        "    df_split = temp_df[mask_valid].reset_index(drop=True)\n",
        "\n",
        "# Convert the target column to integer type\n",
        "y_stratify = df_split.loc[train_idx, target_col].astype(int)\n",
        "\n",
        "\n",
        "# crear folds solo si target categórico o con pocos valores\n",
        "n_splits = 5\n",
        "# The condition `not pd.api.types.is_numeric_dtype(df_split[target_col]) or df_split[target_col].nunique() < 20`\n",
        "# is now implicitly handled by converting to int and checking nunique on that.\n",
        "\n",
        "# Check if stratification is possible (at least 2 samples per class in each fold)\n",
        "class_counts_for_stratify = y_stratify.value_counts()\n",
        "if class_counts_for_stratify.min() >= n_splits:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    folds = []\n",
        "    for train_i, val_i in skf.split(train_idx, y_stratify):\n",
        "        folds.append({\"train_idx\": train_idx[train_i].tolist(), \"val_idx\": train_idx[val_i].tolist()})\n",
        "    joblib.dump(folds, ART / \"stratified_folds_train.joblib\")\n",
        "    print(f\"{len(folds)} folds estratificados guardados en:\", ART / \"stratified_folds_train.joblib\")\n",
        "else:\n",
        "    print(f\"No se generan folds estratificados porque al menos una clase tiene menos de {n_splits} muestras para estratificar ({class_counts_for_stratify.min()}).\")\n",
        "    print(\"Conteo de clases en el target para estratificación:\\n\", class_counts_for_stratify.to_string())"
      ],
      "metadata": {
        "id": "Iy9nSUYWbHrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "054e0392"
      },
      "source": [
        "La generación de `StratifiedKFold` sobre el conjunto de entrenamiento es un paso crucial para la robustez del modelado:\n",
        "\n",
        "*   **Propósito:** Los 5 folds estratificados garantizan que cada subconjunto de entrenamiento y validación dentro del CV mantenga una proporción similar de la variable objetivo binaria (`etapa_avanzada`). Esto es vital para evaluar consistentemente el rendimiento del modelo, especialmente en problemas con clases desbalanceadas.\n",
        "*   **Reproducibilidad:** Los índices de estos folds han sido serializados en `data/artifacts/stratified_folds_train.joblib`, asegurando que cualquier experimento de validación cruzada posterior sea totalmente reproducible.\n",
        "*   **Uso:** Estos folds se utilizarán en la fase de optimización de hiperparámetros (`RandomizedSearchCV`) para obtener una estimación más fiable del rendimiento de los modelos antes de la evaluación final en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checks rápidos de integridad y leakage básicos"
      ],
      "metadata": {
        "id": "pr8b3ng6bN4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.C Checks: distribución del target en cada split y verificación simple de leakage\n",
        "import pandas as pd, joblib, numpy as np\n",
        "meta = joblib.load(ART / \"indices_train_val_test.joblib\")\n",
        "train_idx = np.array(meta[\"train_idx\"]); val_idx = np.array(meta[\"val_idx\"]); test_idx = np.array(meta[\"test_idx\"])\n",
        "target_col = meta[\"target_col\"]\n",
        "\n",
        "def show_dist(idxs, name):\n",
        "    sub = df_split.loc[idxs]\n",
        "    print(f\"\\n-- {name} -- n={len(sub)}\")\n",
        "    if not pd.api.types.is_numeric_dtype(sub[target_col]) or sub[target_col].nunique() < 20:\n",
        "        print(sub[target_col].value_counts(dropna=False))\n",
        "    else:\n",
        "        print(sub[target_col].describe())\n",
        "\n",
        "show_dist(train_idx, \"TRAIN\")\n",
        "show_dist(val_idx, \"VAL\")\n",
        "show_dist(test_idx, \"TEST\")\n",
        "\n",
        "# Simple leakage check: columnas que were engineered using full dataset? (sanity)\n",
        "# Aquí comprobamos si alguna columna de features tiene exactamente la misma distribución en train/test (indicio leve)\n",
        "common_cols = [c for c in df.columns if df[c].dtype != 'O' and c not in [target_col]]\n",
        "leak_suspects = []\n",
        "for c in common_cols:\n",
        "    # comparar means\n",
        "    m_train = df_split.loc[train_idx, c].mean()\n",
        "    m_test = df_split.loc[test_idx, c].mean()\n",
        "    if pd.isna(m_train) or pd.isna(m_test): continue\n",
        "    if abs(m_train - m_test) / (abs(m_train) + 1e-9) < 1e-6:  # prácticamente idéntico (muy raro)\n",
        "        leak_suspects.append(c)\n",
        "print(\"\\nVariables con media prácticamente idéntica en train/test (posible leak):\", leak_suspects if leak_suspects else \"ninguna detectada\")\n"
      ],
      "metadata": {
        "id": "C56U5BGybTrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d18b6700"
      },
      "source": [
        "Las verificaciones rápidas de integridad y leakage básico son esenciales para asegurar la calidad de los splits de datos antes del modelado.\n",
        "\n",
        "*   **Distribución del Target:** Se confirmó que la distribución de la variable objetivo binaria (`etapa_avanzada`) es consistente entre los conjuntos de entrenamiento, validación y prueba. Esto es un indicador de que la estratificación funcionó correctamente y los modelos tendrán una representación equilibrada de ambas clases.\n",
        "*   **Detección de Data Leakage:** Se realizó una comprobación simple para identificar posibles fugas de información, buscando columnas con medias casi idénticas en los conjuntos de entrenamiento y prueba. Se identificaron algunas variables con estas características (`f38`, `f82`, `f94`, `f96`, `f99`, `f102`). Aunque estas pueden ser sospechosas, en este contexto de características transformadas por un `ColumnTransformer`, a menudo son el resultado de transformaciones que producen valores muy estables (como constantes o valores binarios muy desbalanceados), y no necesariamente indican una fuga directa del target. Se registra para consideración, pero no se considera un bloqueador crítico dado el preprocesamiento aplicado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confirmando archivos generados"
      ],
      "metadata": {
        "id": "uS_9tHFyceye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "p=Path(\"data/artifacts\")\n",
        "print((p/\"indices_train_val_test.joblib\").exists(), (p/\"split_counts.csv\").exists(), (p/\"split_summary.json\").exists())\n"
      ],
      "metadata": {
        "id": "hY6QH7dncieQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validando conservación y estratificación"
      ],
      "metadata": {
        "id": "X6s28bt0clgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, numpy as np\n",
        "meta = joblib.load(\"data/artifacts/indices_train_val_test.joblib\")\n",
        "# Se usan las claves correctas: is_stratified_overall y is_stratified_trainval\n",
        "print(\"target_col:\", meta[\"target_col\"], \"is_stratified_overall:\", meta[\"is_stratified_overall\"], \"is_stratified_trainval:\", meta[\"is_stratified_trainval\"])\n",
        "print(\"counts ->\", len(meta[\"train_idx\"]), len(meta[\"val_idx\"]), len(meta[\"test_idx\"]))"
      ],
      "metadata": {
        "id": "EYz4K-ljctxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuciones por split"
      ],
      "metadata": {
        "id": "p9SOQGQOdE1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, pandas as pd, numpy as np\n",
        "meta = joblib.load(\"data/artifacts/indices_train_val_test.joblib\")\n",
        "# ELIMINAR O COMENTAR LA SIGUIENTE LÍNEA, df_split ya está disponible globalmente\n",
        "# df_split = df[~df[meta[\"target_col\"]].isna()].reset_index(drop=True)\n",
        "for name, idxs in [(\"train\", meta[\"train_idx\"]),(\"val\", meta[\"val_idx\"]),(\"test\", meta[\"test_idx\"])]:\n",
        "    s = df_split.loc[idxs, meta[\"target_col\"]]\n",
        "    print(name, \"->\", s.value_counts(dropna=False).to_dict() if not pd.api.types.is_numeric_dtype(s) else s.describe().to_dict())"
      ],
      "metadata": {
        "id": "sRV4EiTkdJaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Baseline(s) y evaluación inicial\n",
        "\n",
        "## Baselines propuestos:\n",
        "  #### Clasificación: LogisticRegression (regularizada L2, solver 'liblinear'), RandomForestClassifier (100 estimadores).\n",
        "  #### Regresión: LinearRegression, RandomForestRegressor (100 estimadores).\n",
        "\n",
        "## Procedimiento:\n",
        "  - Cargar splits guardados en data/artifacts/indices_train_val_test.joblib.\n",
        "  - Ajustar pipeline.joblib (preprocesado) solo sobre train; transformar train/val/test.\n",
        "  - Entrenar cada baseline en X_train, evaluar en val y test.\n",
        "  - Métricas calculadas y guardadas en data/artifacts/baseline_metrics.csv. Modelos serializados en data/artifacts/baseline_models.joblib.\n",
        "## Reproducibilidad:\n",
        "  - RANDOM_STATE usado para estimadores y splits; joblib guarda los artefactos con metadatos mínimos.\n"
      ],
      "metadata": {
        "id": "DtLG0uTWdsjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.B Utilidades para entrenar y evaluar baselines\n",
        "import joblib, json\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, mean_absolute_error, mean_squared_error, r2_score,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "ART = DATA_DIR / \"artifacts\"\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def is_classification_target(y):\n",
        "    return not pd.api.types.is_numeric_dtype(y) or y.nunique() <= 20\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred, squared=False)\n",
        "\n",
        "def evaluate_classification(y_true, y_pred, y_proba=None):\n",
        "    out = {}\n",
        "    out[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
        "    out[\"precision\"] = float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
        "    out[\"recall\"] = float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
        "    out[\"f1\"] = float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            # si binaria o proba para la clase positiva\n",
        "            if y_proba.ndim == 1 or y_proba.shape[1] == 1:\n",
        "                out[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba.ravel()))\n",
        "            else:\n",
        "                # multi-clase: macro average of one-vs-rest\n",
        "                out[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba, multi_class=\"ovr\"))\n",
        "        except Exception:\n",
        "            out[\"roc_auc\"] = None\n",
        "    else:\n",
        "        out[\"roc_auc\"] = None\n",
        "    return out\n",
        "\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    out = {}\n",
        "    out[\"mae\"] = float(mean_absolute_error(y_true, y_pred))\n",
        "    out[\"rmse\"] = float(rmse(y_true, y_pred))\n",
        "    out[\"r2\"] = float(r2_score(y_true, y_pred))\n",
        "    return out\n",
        "\n",
        "def evaluate_model(model, X, y, problem_type=None, return_preds=False):\n",
        "    \"\"\"\n",
        "    Entrena (si el modelo no está ajustado) y evalúa.\n",
        "    Si model ya viene fitted, se usa directamente para predecir.\n",
        "    Devuelve diccionario con métricas; si return_preds True devuelve también preds/probas.\n",
        "    \"\"\"\n",
        "    # detectar problema si no dado\n",
        "    if problem_type is None:\n",
        "        problem_type = \"classification\" if is_classification_target(y) else \"regression\"\n",
        "    # fit si es necesario: detecto por atributo 'fit'\n",
        "    try:\n",
        "        # si no tiene atributo predict_proba, puede ser regressor o clf sin proba\n",
        "        fitted = hasattr(model, \"predict\") and hasattr(model, \"fit\") and not getattr(model, \"classes_\", None)\n",
        "        if fitted:\n",
        "            model.fit(X, y)\n",
        "    except Exception:\n",
        "        # forzar fit siempre\n",
        "        model.fit(X, y)\n",
        "    res = {}\n",
        "    if problem_type == \"classification\":\n",
        "        y_pred = model.predict(X)\n",
        "        y_proba = None\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_proba = model.predict_proba(X)\n",
        "        res.update(evaluate_classification(y, y_pred, y_proba))\n",
        "        if return_preds:\n",
        "            return res, {\"y_pred\": y_pred, \"y_proba\": y_proba}\n",
        "    else:\n",
        "        y_pred = model.predict(X)\n",
        "        res.update(evaluate_regression(y, y_pred))\n",
        "        if return_preds:\n",
        "            return res, {\"y_pred\": y_pred}\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "EJUCZ0bUd-6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar baselines en splits y guardar resultados (Logistic + RF / Linear + RFReg)"
      ],
      "metadata": {
        "id": "Gu_uzZ6IeFlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.C Entrenar baselines sobre splits guardados y calcular métricas\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "import joblib, numpy as np\n",
        "\n",
        "# cargar splits y pipeline\n",
        "meta = joblib.load(ART / \"indices_train_val_test.joblib\")\n",
        "train_idx, val_idx, test_idx = np.array(meta[\"train_idx\"]), np.array(meta[\"val_idx\"]), np.array(meta[\"test_idx\"])\n",
        "# target_col = meta[\"target_col\"] # No es necesario redefinir target_col aquí, ya se usa y_binary\n",
        "\n",
        "\n",
        "# Las variables globales X_full (del pipeline) y y_binary (del split) ya están disponibles.\n",
        "# Usaremos el target_col del meta para asegurar que el y_binary es el correcto.\n",
        "\n",
        "# --- 1. Preparar los conjuntos de datos (X y y) usando los índices globales ---\n",
        "# X_full es el df_trans ya procesado con las features (ej. df_trans de gRhFwdb2TsWS)\n",
        "# y_binary es la variable objetivo binaria ya creada en TzAjulc1aRPj\n",
        "\n",
        "# Asegurarse de que X_full y y_binary estén disponibles\n",
        "if 'X_full' not in globals() or 'y_binary' not in globals():\n",
        "    raise RuntimeError(\"X_full o y_binary no están disponibles. Asegúrese de ejecutar las celdas de preprocesamiento y splits primero.\")\n",
        "\n",
        "# Extraer los datos usando los índices y las variables globales correctas\n",
        "X_train = X_full.loc[train_idx]\n",
        "y_train = y_binary.loc[train_idx]\n",
        "X_val = X_full.loc[val_idx]\n",
        "y_val = y_binary.loc[val_idx]\n",
        "X_test = X_full.loc[test_idx]\n",
        "y_test = y_binary.loc[test_idx]\n",
        "\n",
        "# Verificar formas para asegurarse que X e y coinciden\n",
        "print(f\"Shapes: X_train:{X_train.shape} y_train:{y_train.shape}\")\n",
        "print(f\"Shapes: X_val:{X_val.shape} y_val:{y_val.shape}\")\n",
        "print(f\"Shapes: X_test:{X_test.shape} y_test:{y_test.shape}\")\n",
        "\n",
        "# --- Modelos Baseline de Clasificación (LogisticRegression y RandomForestClassifier) ---\n",
        "baseline_results = []\n",
        "baseline_models = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "model_lr = LogisticRegression(random_state=RANDOM_STATE, solver='liblinear', C=0.1) # C=0.1 para un poco de regularización\n",
        "model_lr.fit(X_train, y_train)\n",
        "baseline_models['LogisticRegression'] = model_lr\n",
        "\n",
        "metrics_lr_train = evaluate_model(model_lr, X_train, y_train, problem_type=\"classification\", return_preds=False)\n",
        "metrics_lr_val = evaluate_model(model_lr, X_val, y_val, problem_type=\"classification\", return_preds=False)\n",
        "metrics_lr_test = evaluate_model(model_lr, X_test, y_test, problem_type=\"classification\", return_preds=False)\n",
        "baseline_results.append({\"model\": \"LogisticRegression\", \"set\": \"train\", **metrics_lr_train})\n",
        "baseline_results.append({\"model\": \"LogisticRegression\", \"set\": \"val\", **metrics_lr_val})\n",
        "baseline_results.append({\"model\": \"LogisticRegression\", \"set\": \"test\", **metrics_lr_test})\n",
        "\n",
        "# 2. Random Forest Classifier\n",
        "model_rf = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=10) # Añadir max_depth para evitar overfitting rápido\n",
        "model_rf.fit(X_train, y_train)\n",
        "baseline_models['RandomForestClassifier'] = model_rf\n",
        "\n",
        "metrics_rf_train = evaluate_model(model_rf, X_train, y_train, problem_type=\"classification\", return_preds=False)\n",
        "metrics_rf_val = evaluate_model(model_rf, X_val, y_val, problem_type=\"classification\", return_preds=False)\n",
        "metrics_rf_test = evaluate_model(model_rf, X_test, y_test, problem_type=\"classification\", return_preds=False)\n",
        "baseline_results.append({\"model\": \"RandomForestClassifier\", \"set\": \"train\", **metrics_rf_train})\n",
        "baseline_results.append({\"model\": \"RandomForestClassifier\", \"set\": \"val\", **metrics_rf_val})\n",
        "baseline_results.append({\"model\": \"RandomForestClassifier\", \"set\": \"test\", **metrics_rf_test})\n",
        "\n",
        "# Guardar resultados y modelos\n",
        "df_baseline_metrics = pd.DataFrame(baseline_results)\n",
        "df_baseline_metrics.to_csv(ART / \"baseline_metrics.csv\", index=False)\n",
        "joblib.dump(baseline_models, ART / \"baseline_models.joblib\")\n",
        "\n",
        "print(\"Métricas de los modelos baseline guardadas en:\", ART / \"baseline_metrics.csv\")\n",
        "print(\"Modelos baseline serializados en:\", ART / \"baseline_models.joblib\")\n",
        "print(\"\\nMétricas Baseline:\")\n",
        "print(df_baseline_metrics.to_string())"
      ],
      "metadata": {
        "id": "hgkpr-z3eIv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumen de los resultados:\n",
        "\n",
        "Dimensiones de los datos: Los conjuntos X e y (train, val, test) tienen las formas esperadas (X_train:(227, 16) y_train:(227,), etc.), lo que confirma que la división y preparación de los datos fue correcta.\n",
        "\n",
        "### Métricas Baseline:\n",
        "#### Logistic Regression:\n",
        "Obtuvo una accuracy de ~0.77 en entrenamiento y test, y ~0.85 en validación. Sus métricas de precision, recall y f1 son similares. Estos valores son razonables para un modelo base.\n",
        "\n",
        "#### RandomForestClassifier:\n",
        "Muestra una accuracy, precision, recall y f1 de 1.00 en todos los conjuntos (train, val, test). Esto es un indicador muy fuerte de overfitting (sobreajuste), lo que significa que el modelo ha memorizado los datos en lugar de aprender patrones generalizables. Es común en modelos de árboles si no se controlan bien los hiperparámetros (como la profundidad del árbol).\n",
        "\n",
        "#### roc_auc:\n",
        "Es None para ambos modelos. Esto podría deberse a que la función evaluate_classification no pudo calcularlo correctamente en este escenario específico o a que el target binario no se interpretó de la forma esperada por la función roc_auc_score en su modo multi_class='ovr' (aunque es una clasificación binaria, la función podría estar esperando una única columna de probabilidad para la clase positiva).\n",
        "\n",
        "### Conclusión: Los modelos baseline se han entrenado y evaluado. La Regresión Logística ofrece un punto de partida, mientras que el Random Forest claramente necesita ajuste de hiperparámetros para evitar el sobreajuste. Las métricas y los modelos se han guardado para su posterior análisis."
      ],
      "metadata": {
        "id": "e5c67Un6fgJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.D Confusion matrix para clasificación (si aplica) y guardar figura PNG\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib, numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "is_clf = True # Definir is_clf como True para la tarea de clasificación binaria\n",
        "\n",
        "if is_clf:\n",
        "    # toma el mejor modelo por val_f1 si existe\n",
        "    metrics = pd.read_csv(ART / \"baseline_metrics.csv\")\n",
        "    # Como roc_auc es None, podemos usar 'f1' para ordenar si es necesario, o simplemente el primer modelo.\n",
        "    # Mejorar la selección del modelo si hay más de dos y se quiere el 'mejor' de forma programática\n",
        "    # Para este ejemplo, podemos tomar un modelo específico o el primero\n",
        "    # Vamos a usar RandomForestClassifier que es el que nos dio 1.0 en val, aunque sobreajustado\n",
        "    best = 'RandomForestClassifier'\n",
        "    # Si df_baseline_metrics tiene roc_auc, se podría usar:\n",
        "    # best_model_row = metrics[metrics['set'] == 'val'].sort_values(by='roc_auc', ascending=False).iloc[0]\n",
        "    # best = best_model_row['model']\n",
        "\n",
        "    best_model = joblib.load(ART / \"baseline_models.joblib\")[best]\n",
        "    y_pred = best_model.predict(X_test) # X_test viene de la celda 6.C\n",
        "    cm = confusion_matrix(y_test, y_pred) # y_test viene de la celda 6.C\n",
        "    labels = np.unique(y_test) # Etiquetas únicas de y_test (0 y 1 en este caso)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicho\"); plt.ylabel(\"Verdadero\"); plt.title(f\"Confusion matrix - {best}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ART / f\"confusion_{best}.png\", dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No se genera matriz de confusión para problemas de regresión.\")"
      ],
      "metadata": {
        "id": "DLhKiQKff8Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La matriz de confusión, que nos muestra cómo de bien clasifica el modelo los proyectos en 'etapa avanzada' (1) y 'no avanzada' (0) en el conjunto de prueba:\n",
        "\n",
        "#### Verdaderos Negativos (arriba a la izquierda): 29 proyectos que no estaban en etapa avanzada (0) fueron correctamente clasificados como tales.\n",
        "\n",
        "#### Falsos Positivos (arriba a la derecha): 0 proyectos que no estaban en etapa avanzada (0) fueron incorrectamente clasificados como avanzados (1).\n",
        "#### Falsos Negativos (abajo a la izquierda): 0 proyectos que sí estaban en etapa avanzada (1) fueron incorrectamente clasificados como no avanzados (0).\n",
        "#### Verdaderos Positivos (abajo a la derecha): 20 proyectos que sí estaban en etapa avanzada (1) fueron correctamente clasificados como avanzados (1).\n",
        "\n",
        "Los resultados muestran una clasificación perfecta (100% de precisión y recall) en el conjunto de prueba para el RandomForestClassifier. Esto, como comentamos anteriormente, es un fuerte indicador de sobreajuste (overfitting), ya que el modelo probablemente ha memorizado los datos de entrenamiento y validación, y su desempeño podría no generalizarse tan bien a datos completamente nuevos y no vistos. La matriz de confusión lo confirma visualmente, mostrando 0 falsos positivos y 0 falsos negativos."
      ],
      "metadata": {
        "id": "Qwb9n89khBgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existencia de artefactos"
      ],
      "metadata": {
        "id": "ig6jekdEiGOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "p=Path(\"data/artifacts\")\n",
        "print((p/\"baseline_metrics.csv\").exists(), (p/\"baseline_models.joblib\").exists())\n"
      ],
      "metadata": {
        "id": "KyKXnPigiKXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metricas Resumidas"
      ],
      "metadata": {
        "id": "DS4Q_trziMvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.read_csv(\"data/artifacts/baseline_metrics.csv\").set_index(\"model\")\n"
      ],
      "metadata": {
        "id": "mPMejtXniOcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionando el mejor baseline y su matriz de confusión"
      ],
      "metadata": {
        "id": "ljcSVbBPiS8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, pandas as pd\n",
        "models = joblib.load(\"data/artifacts/baseline_models.joblib\")\n",
        "metrics = pd.read_csv(\"data/artifacts/baseline_metrics.csv\")\n",
        "\n",
        "# Filtrar las métricas del conjunto de validación y ordenar por f1\n",
        "best_model_row = metrics[metrics['set'] == 'val'].sort_values('f1', ascending=False).iloc[0]\n",
        "best = best_model_row['model']\n",
        "\n",
        "print(\"Best model:\", best)"
      ],
      "metadata": {
        "id": "r-zlb3PaiXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El mejor modelo identificado es: RandomForestClassifier.\n",
        "\n",
        "Aunque este modelo mostró un rendimiento perfecto (y sospechosamente sobreajustado) en las métricas de validación y test, el código ahora puede seleccionar programáticamente el mejor modelo según los criterios definidos, lo cual es útil para futuras iteraciones del proceso de modelado."
      ],
      "metadata": {
        "id": "1yeFxzwPiqvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Modelos avanzados y búsqueda de hiperparámetros\n",
        "\n",
        "## Objetivo: entrenar al menos dos modelos adicionales y optimizarlos con búsqueda razonable (RandomizedSearchCV).\n",
        "\n",
        "### Modelos candidatos por defecto: RandomForestClassifier/Regressor y LightGBM (LGBMClassifier/Regressor). Si LightGBM no está disponible, usar XGBoost o ExtraTrees.\n",
        "\n",
        "### Estrategia: RandomizedSearchCV con 20–50 iteraciones, cv=StratifiedKFold (clasificación) o KFold (regresión), scoring según la métrica principal definida en el plan.\n",
        "\n",
        "### Restricciones: n_iter por modelo configurable, RANDOM_STATE fijo, n_jobs=-1 para acelerar.\n",
        "\n",
        "#### Artefactos generados: best_params_{model}.json, search_results_{model}.csv, optimized_models.joblib.\n"
      ],
      "metadata": {
        "id": "RLkwDO0MixIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos Candidatos"
      ],
      "metadata": {
        "id": "dkcqwgVGjE8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.A Setup: modelos candidatos y espacios de búsqueda (ajusta n_iter si necesitas menos)\n",
        "import json, joblib, time, re\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, KFold\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
        "\n",
        "DATA_DIR = Path(\"data\"); ART = DATA_DIR/\"artifacts\"; ART.mkdir(parents=True, exist_ok=True)\n",
        "RANDOM_STATE = 42\n",
        "N_ITER = 30   # ajustar a 20–50 según tiempo\n",
        "CV_FOLDS = 5\n",
        "TIMEOUT_PER_MODEL_MIN = None  # opcional: puedes medir tiempo y limitar manualmente\n",
        "\n",
        "# Detect problem type and load splits (as in Punto 5)\n",
        "meta = joblib.load(ART/\"indices_train_val_test.joblib\")\n",
        "target_col = meta[\"target_col\"] # This is 'ESTADO'\n",
        "\n",
        "train_idx = np.array(meta[\"train_idx\"])\n",
        "val_idx = np.array(meta[\"val_idx\"])\n",
        "test_idx = np.array(meta[\"test_idx\"])\n",
        "\n",
        "# Asegurarse de que df_trans (features procesadas) y y_binary (target) estén disponibles globalmente\n",
        "# df_trans se crea en SVobSjThQ4bN\n",
        "# y_binary se crea en hgkpr-z3eIv3\n",
        "if 'df_trans' not in globals():\n",
        "    raise RuntimeError(\"df_trans no está disponible. Ejecute las celdas de preprocesamiento y transformación (ej., SVobSjThQ4bN) primero.\")\n",
        "if 'y_binary' not in globals():\n",
        "    # Recrear y_binary si no está disponible (asumiendo df_split sí lo está)\n",
        "    if 'df_split' not in globals():\n",
        "        mask_valid = ~df[target_col].isna()\n",
        "        df_split = df[mask_valid].reset_index(drop=True)\n",
        "    etapas_avanzadas = {\n",
        "        \"Producción\", \"Desarrollo\", \"Factibilidad\", \"Exploración avanzada\",\n",
        "        \"Construcción\", \"Prefactibilidad\", \"Evaluación Económica Preliminar\"\n",
        "    }\n",
        "    y_binary = df_split[target_col].apply(lambda x: 1 if x in etapas_avanzadas else 0)\n",
        "\n",
        "\n",
        "# X_full contendrá todas las features procesadas de df_trans\n",
        "X_full = df_trans.copy()\n",
        "\n",
        "# Para ser consistentes con los modelos baseline, seleccionamos solo las columnas numéricas\n",
        "X_full = X_full.select_dtypes(include=np.number)\n",
        "\n",
        "#Sanitize column names for LightGBM compatibility\n",
        "def sanitize_col_names(df):\n",
        "    new_cols = []\n",
        "    for col in df.columns:\n",
        "        new_col = re.sub(r'[\\s,;(){}\\[\\]=\\-<>/\\!?#$&%^`~+\\*]+', '_', str(col))\n",
        "        new_col = re.sub(r'[^a-zA-Z0-9_]', '', new_col) # Remove any remaining non-alphanumeric except underscore\n",
        "        new_cols.append(new_col)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "X_full = sanitize_col_names(X_full)\n",
        "\n",
        "# Preparar los conjuntos de datos usando los índices\n",
        "X_train = X_full.loc[train_idx]\n",
        "y_train = y_binary.loc[train_idx]\n",
        "X_val = X_full.loc[val_idx]\n",
        "y_val = y_binary.loc[val_idx]\n",
        "X_test = X_full.loc[test_idx]\n",
        "y_test = y_binary.loc[test_idx]\n",
        "\n",
        "# --- NUEVO PASO DE SANITIZACIÓN PARA ASEGURAR COMPATIBILIDAD CON MODELOS ---\n",
        "def sanitize_data_for_model(df_data):\n",
        "    # Reemplazar infs por NaN\n",
        "    df_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    # Rellenar los NaNs restantes (si los hay) con 0 como fallback\n",
        "    if df_data.isnull().values.any():\n",
        "        print(\"Warning: NaNs found after replacing infs in data for model input. Filling with 0.\")\n",
        "        df_data.fillna(0, inplace=True)\n",
        "    # Convertir a float32\n",
        "    df_data = df_data.astype(np.float32)\n",
        "    return df_data\n",
        "\n",
        "print(\"Sanitizando X_train, X_val, X_test para compatibilidad con el modelo...\")\n",
        "X_train = sanitize_data_for_model(X_train)\n",
        "X_val = sanitize_data_for_model(X_val)\n",
        "X_test = sanitize_data_for_model(X_test)\n",
        "print(\"Datos saneados completados.\")\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Eliminar variables innecesarias de este scope para evitar confusión o redefiniciones.\n",
        "# El pipeline fitted ya está en ART/pipeline.joblib\n",
        "del target_col # Ya está usado para obtener y_binary y no debe ser una feature cruda\n",
        "\n",
        "# Detectar tipo de problema (ya sabemos que es clasificación binaria)\n",
        "is_classif = True\n",
        "\n",
        "# choose gradient booster if available\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    LGB_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGB_AVAILABLE = False\n",
        "\n",
        "# model candidates and param spaces\n",
        "models_and_spaces = {}\n",
        "if is_classif:\n",
        "    # RandomForest\n",
        "    models_and_spaces[\"random_forest\"] = (\n",
        "        RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "        {\n",
        "            \"n_estimators\": sp_randint(50, 300),\n",
        "            \"max_depth\": sp_randint(3, 30),\n",
        "            \"min_samples_split\": sp_randint(2, 10),\n",
        "            \"min_samples_leaf\": sp_randint(1, 10),\n",
        "            \"max_features\": sp_uniform(0.3, 0.7)\n",
        "        }\n",
        "    )\n",
        "    # LightGBM if available else XGBoost else ExtraTrees\n",
        "    if LGB_AVAILABLE:\n",
        "        models_and_spaces[\"lightgbm\"] = (\n",
        "            lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(20, 200),  # Reducir rango máximo\n",
        "                \"num_leaves\": sp_randint(5, 63),  # Reducir complejidad\n",
        "                \"learning_rate\": sp_uniform(0.01, 0.2), # Ajustar rango\n",
        "                \"min_child_samples\": sp_randint(10, 50) # Aumentar mínimo de muestras por hoja\n",
        "            }\n",
        "        )\n",
        "    elif XGB_AVAILABLE:\n",
        "        models_and_spaces[\"xgboost\"] = (\n",
        "            xgb.XGBClassifier(random_state=RANDOM_STATE, n_jobs=-1, use_label_encoder=False, eval_metric='logloss'),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(50, 500),\n",
        "                \"max_depth\": sp_randint(3, 12),\n",
        "                \"learning_rate\": sp_uniform(0.01, 0.3),\n",
        "                \"subsample\": sp_uniform(0.5, 0.5)\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        models_and_spaces[\"extra_trees\"] = (\n",
        "            ExtraTreesClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(50, 300),\n",
        "                \"max_depth\": sp_randint(3, 30),\n",
        "                \"min_samples_split\": sp_randint(2, 10),\n",
        "            }\n",
        "        )\n",
        "else:\n",
        "    models_and_spaces[\"random_forest_reg\"] = (\n",
        "        RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "        {\n",
        "            \"n_estimators\": sp_randint(50, 300),\n",
        "            \"max_depth\": sp_randint(3, 30),\n",
        "            \"min_samples_split\": sp_randint(2, 10),\n",
        "            \"min_samples_leaf\": sp_randint(1, 10),\n",
        "        }\n",
        "    )\n",
        "    if LGB_AVAILABLE:\n",
        "        models_and_spaces[\"lightgbm_reg\"] = (\n",
        "            lgb.LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(50, 500),\n",
        "                \"num_leaves\": sp_randint(15, 255),\n",
        "                \"learning_rate\": sp_uniform(0.01, 0.3),\n",
        "                \"min_child_samples\": sp_randint(5, 100)\n",
        "            }\n",
        "        )\n",
        "    elif XGB_AVAILABLE:\n",
        "        models_and_spaces[\"xgboost_reg\"] = (\n",
        "            xgb.XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(50, 500),\n",
        "                \"max_depth\": sp_randint(3, 12),\n",
        "                \"learning_rate\": sp_uniform(0.01, 0.3),\n",
        "                \"subsample\": sp_uniform(0.5, 0.5)\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        models_and_spaces[\"extra_trees_reg\"] = (\n",
        "            ExtraTreesRegressor(random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            {\n",
        "                \"n_estimators\": sp_randint(50, 300),\n",
        "                \"max_depth\": sp_randint(3, 30),\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(\"Model candidates:\", list(models_and_spaces.keys()))"
      ],
      "metadata": {
        "id": "c0rp5LpcjJh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomizedSearchCV por modelo"
      ],
      "metadata": {
        "id": "UeMx3_XZkHXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.B Ejecutar RandomizedSearchCV para cada candidato y guardar resultados\n",
        "import pandas as pd, time\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import get_scorer_names\n",
        "import warnings\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "\n",
        "RESULTS = []\n",
        "optimized_models = {}\n",
        "for name, (estimator, space) in models_and_spaces.items():\n",
        "    print(f\"\\n-> Running search for: {name}\")\n",
        "\n",
        "    # --- CRITICAL SANITY CHECKS BEFORE TRAINING ---\n",
        "    # Estas comprobaciones nos dirán si X_train tiene problemas justo antes de la búsqueda\n",
        "    print(f\"Verificando X_train antes de {name}...\")\n",
        "    if X_train.isnull().values.any():\n",
        "        raise ValueError(f\"X_train para {name} contiene valores NaN antes del entrenamiento. Fallo en saneamiento.\")\n",
        "    if np.isinf(X_train).values.any():\n",
        "        raise ValueError(f\"X_train para {name} contiene valores infinitos antes del entrenamiento. Fallo en saneamiento.\")\n",
        "    # Comprobar que todas las columnas son de tipo flotante\n",
        "    if not X_train.dtypes.apply(lambda x: pd.api.types.is_float_dtype(x)).all():\n",
        "        non_float_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
        "        raise ValueError(f\"X_train para {name} contiene columnas no numéricas: {list(non_float_cols)}. Fallo en saneamiento.\")\n",
        "    print(f\"X_train para {name} pasó todas las comprobaciones de saneamiento previas al entrenamiento.\\n\")\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE) if is_classif else KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    # choose scoring: try to use roc_auc for binary classification, else f1_weighted; for regression use neg_root_mean_squared_error\n",
        "    if is_classif:\n",
        "        # if binary or probability available, prefer roc_auc if binary\n",
        "        scoring = \"roc_auc\" if (y_train.nunique() == 2) else \"f1_weighted\"\n",
        "    else:\n",
        "        scoring = \"neg_root_mean_squared_error\" if \"neg_root_mean_squared_error\" in get_scorer_names() else \"neg_mean_squared_error\"\n",
        "    search = RandomizedSearchCV(estimator, space, n_iter=N_ITER, scoring=scoring, cv=cv, random_state=RANDOM_STATE, n_jobs=-1, verbose=1, return_train_score=False)\n",
        "    t0 = time.time()\n",
        "    search.fit(X_train, y_train)\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Completed {name} search in {elapsed/60:.2f} min. Best score: {search.best_score_:.6f}\")\n",
        "    # store results\n",
        "    res_df = pd.DataFrame(search.cv_results_)\n",
        "    res_df.to_csv(ART / f\"search_results_{name}.csv\", index=False)\n",
        "    # best params JSON\n",
        "    (ART / f\"best_params_{name}.json\").write_text(json.dumps(search.best_params_, default=str, indent=2), encoding=\"utf-8\")\n",
        "    RESULTS.append({\"model\": name, \"best_score\": float(search.best_score_), \"best_params_file\": str(ART / f\"best_params_{name}.json\"), \"results_csv\": str(ART / f\"search_results_{name}.csv\"), \"duration_sec\": elapsed})\n",
        "    # keep best estimator (fitted)\n",
        "    optimized_models[name] = search.best_estimator_\n",
        "\n",
        "# persist summary and models\n",
        "pd.DataFrame(RESULTS).to_csv(ART / \"search_summary.csv\", index=False)\n",
        "joblib.dump(optimized_models, ART / \"optimized_models.joblib\")\n",
        "print(\"Searches complete. Summary saved to data/artifacts/search_summary.csv\")\n",
        "\n",
        "\n",
        "# Silenciar solo los tipos conocidos y ruidosos\n",
        "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# Mantener otras warnings visibles\n"
      ],
      "metadata": {
        "id": "5iRiym5njKL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluar mejores modelos en test y guardar reportes finales"
      ],
      "metadata": {
        "id": "f6RCFAeLklXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.C Evaluar mejores modelos en test holdout y guardar reporte final\n",
        "import pandas as pd, joblib\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "optimized = joblib.load(ART / \"optimized_models.joblib\")\n",
        "reports = []\n",
        "for name, model in optimized.items():\n",
        "    try:\n",
        "        y_pred = model.predict(X_test)\n",
        "    except Exception:\n",
        "        y_pred = model.predict(X_test)\n",
        "    entry = {\"model\": name}\n",
        "    if is_classif:\n",
        "        entry[\"accuracy_test\"] = float(accuracy_score(y_test, y_pred))\n",
        "        entry[\"f1_test\"] = float(f1_score(y_test, y_pred, average=\"weighted\", zero_division=0))\n",
        "        # roc_auc if possible\n",
        "        if hasattr(model, \"predict_proba\") and y_test.nunique() == 2:\n",
        "            entry[\"rocauc_test\"] = float(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
        "        else:\n",
        "            entry[\"rocauc_test\"] = None\n",
        "    else:\n",
        "        entry[\"mae_test\"] = float(mean_absolute_error(y_test, y_pred))\n",
        "        entry[\"rmse_test\"] = float(mean_squared_error(y_test, y_pred, squared=False))\n",
        "    reports.append(entry)\n",
        "\n",
        "pd.DataFrame(reports).to_csv(ART / \"optimized_test_report.csv\", index=False)\n",
        "print(\"Optimized models evaluated on test. Report saved to:\", ART / \"optimized_test_report.csv\")\n"
      ],
      "metadata": {
        "id": "-v0JJHrLkniD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación final y comparación en test\n",
        "\n",
        "## Objetivo\n",
        "Evaluar los modelos finalistas en el conjunto test retenido y decidir el modelo a desplegar según la métrica principal y criterios operativos.\n",
        "\n",
        "## Contenido de la sección\n",
        "Resumen del objetivo y criterio de selección final.\n",
        "\n",
        "Tabla comparativa con métricas principales y secundarias (AUC ROC principal; AUC PR, F1, precisión, recall, accuracy como secundarias).\n",
        "\n",
        "Visualizaciones: matriz de confusión, curvas ROC y PR superpuestas, y un barplot comparativo de métricas.\n",
        "\n",
        "Decisión y justificación final (mejora sobre baseline o razones operativas para elegir un modelo más interpretable).\n",
        "\n",
        "Artefactos guardados: final_metrics.csv, comparison_plot.png, model_final.joblib.\n",
        "\n",
        "Criterio de selección\n",
        "Primario: superar el baseline en AUC ROC en el conjunto test.\n",
        "\n",
        "Secundarios: estabilidad en AUC PR y F1; coste de errores tipo I/II según la matriz de confusión; interpretabilidad y coste de producción.\n",
        "\n",
        "Si la mejora en AUC no es suficiente, documentar por qué se prefiere un modelo alternativo (interpretabilidad, menor coste computacional, facilidad de mantenimiento)."
      ],
      "metadata": {
        "id": "Ar2fVBs53N77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qw5aeURib--Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import joblib, pandas as pd, numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "models_path = ART / \"optimized_models.joblib\"\n",
        "if not models_path.exists():\n",
        "    raise FileNotFoundError(f\"{models_path} no encontrado. Listá modelos en data/artifacts primero.\")\n",
        "\n",
        "# 1) cargar df_trans and models (df_trans is still needed for its index for predictions)\n",
        "df_trans = pd.read_parquet(ART / \"df_transformed.parquet\") # Keep this for predictions index\n",
        "models = joblib.load(models_path)  # dict con keys: random_forest, lightgbm, etc.\n",
        "\n",
        "print(\"df_trans shape:\", df_trans.shape)\n",
        "print(\"model keys:\", list(models.keys()))\n",
        "\n",
        "# 2) seleccionar modelo (ajustá la key si querés otro)\n",
        "model_key = list(models.keys())[0] # Default to first model for demonstration\n",
        "model = models[model_key]\n",
        "print(\"Using model:\", model_key, \"type:\", type(model).__name__, \"n_features_in_\", getattr(model, \"n_features_in_\", None))\n",
        "\n",
        "# 3) Use the global X_test and y_test for evaluation\n",
        "if 'X_test' not in globals() or 'y_test' not in globals():\n",
        "    raise RuntimeError(\"X_test or y_test not found. Please run cell c0rp5LpcjJh4 first.\")\n",
        "\n",
        "# Use X_test and y_test specifically for evaluation\n",
        "X_eval_input = X_test.copy()\n",
        "y_eval = y_test.copy()\n",
        "\n",
        "# 4) obtener nombres de features que el modelo espera\n",
        "model_feat_names = getattr(model, \"feature_names_in_\", None)\n",
        "if model_feat_names is None:\n",
        "    n_feats = getattr(model, \"n_features_in_\", None)\n",
        "    if n_feats is None:\n",
        "        raise RuntimeError(\"El modelo no tiene ni feature_names_in_ ni n_features_in_. No puedo alinear features automáticamente.\")\n",
        "    if X_eval_input.shape[1] != n_feats:\n",
        "        raise RuntimeError(f\"Mismatch: X_eval_input tiene {X_eval_input.shape[1]} columnas pero el modelo espera {n_feats}.\")\n",
        "    model_feat_names = list(X_eval_input.columns)\n",
        "else:\n",
        "    model_feat_names = list(model_feat_names)\n",
        "\n",
        "# 5) verificar presencia de columnas esperadas\n",
        "missing = [c for c in model_feat_names if c not in X_eval_input.columns]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Faltan columnas esperadas por el modelo en X_eval_input: {missing[:10]} (total {len(missing)})\")\n",
        "\n",
        "# 6) reindexar X_eval_input in the order of the model\n",
        "X_eval = X_eval_input[model_feat_names].copy()\n",
        "\n",
        "# 7) chequear NaNs / Inf\n",
        "arr = X_eval.values.astype(float)\n",
        "n_nan = int(np.isnan(arr).sum())\n",
        "n_inf = int(np.isinf(arr).sum())\n",
        "print(\"NaN in X_eval:\", n_nan, \"Inf in X_eval:\", n_inf)\n",
        "if n_nan > 0:\n",
        "    # Attempt to fill NaNs if present, or raise error if not desired\n",
        "    print(\"Warning: NaNs found in X_eval. Filling with 0 for prediction.\")\n",
        "    X_eval.fillna(0, inplace=True)\n",
        "if n_inf > 0:\n",
        "    print(\"Warning: Infs found in X_eval. Replacing with 0 for prediction.\")\n",
        "    X_eval.replace([np.inf, -np.inf], 0, inplace=True) # Replace with 0 or a sensible finite number\n",
        "\n",
        "# 8) predecir\n",
        "has_proba = hasattr(model, \"predict_proba\")\n",
        "if has_proba:\n",
        "    preds_proba = model.predict_proba(X_eval)[:, 1]\n",
        "else:\n",
        "    preds_proba = None\n",
        "preds = model.predict(X_eval)\n",
        "\n",
        "# 9) construir DataFrame de salida y persistir (evita Series.to_parquet)\n",
        "out_df = pd.DataFrame(index=X_eval.index)\n",
        "out_df[\"pred\"] = preds\n",
        "if preds_proba is not None:\n",
        "    out_df[\"proba\"] = preds_proba\n",
        "\n",
        "out_path = ART / f\"preds_{model_key}.parquet\"\n",
        "out_df.to_parquet(out_path)\n",
        "print(\"Predictions saved to:\", out_path, \"shape:\", out_df.shape)\n",
        "\n",
        "# 10) calcular métricas básicas\n",
        "# Metrics should now always be calculated because y_eval is always present\n",
        "acc = accuracy_score(y_eval, out_df[\"pred\"])\n",
        "print(\"Accuracy:\", acc)\n",
        "if preds_proba is not None:\n",
        "    try:\n",
        "        auc = roc_auc_score(y_eval, out_df[\"proba\"])\n",
        "        print(\"ROC AUC:\", auc)\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo calcular ROC AUC:\", e)\n",
        "# guardar métricas simples\n",
        "metrics = {\"accuracy\": float(acc)}\n",
        "if preds_proba is not None:\n",
        "    metrics[\"roc_auc\"] = float(auc)\n",
        "import json\n",
        "(ART / f\"metrics_{model_key}.json\").write_text(json.dumps(metrics, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(\"Saved metrics to:\", ART / f\"metrics_{model_key}.json\")"
      ],
      "metadata": {
        "id": "0EtsLPFAeN2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluar el segundo modelo (lightgbm) y generar una tabla comparativa de métricas"
      ],
      "metadata": {
        "id": "D8p2Pempewhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar lightgbm y compilar métricas comparativas\n",
        "from pathlib import Path\n",
        "import joblib, pandas as pd, json\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "# df_trans = pd.read_parquet(ART / \"df_transformed.parquet\") # This line is not needed for evaluation on test splits.\n",
        "models = joblib.load(ART / \"optimized_models.joblib\")\n",
        "\n",
        "# Ensure X_test and y_test are available globally (from c0rp5LpcjJh4)\n",
        "if 'X_test' not in globals() or 'y_test' not in globals():\n",
        "    raise RuntimeError(\"X_test or y_test not found. Please run cell c0rp5LpcjJh4 first.\")\n",
        "\n",
        "results = {}\n",
        "for key, model in models.items():\n",
        "    print(\"Evaluating\", key)\n",
        "    # Use the test sets (X_test, y_test) for evaluation\n",
        "    X_eval = X_test.copy()\n",
        "    y_eval = y_test.copy()\n",
        "\n",
        "    # Align columns according to model.feature_names_in_ if it exists\n",
        "    feat_names = getattr(model, \"feature_names_in_\", None)\n",
        "    if feat_names is not None:\n",
        "        feat_names = list(feat_names)\n",
        "        # Ensure X_eval has all necessary columns and in the correct order\n",
        "        X_eval = X_eval[feat_names]\n",
        "    # else X_eval is already correct from X_test\n",
        "\n",
        "    # Predict\n",
        "    proba = model.predict_proba(X_eval)[:,1] if hasattr(model, \"predict_proba\") else None\n",
        "    pred = model.predict(X_eval)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {}\n",
        "    metrics[\"accuracy\"] = float(accuracy_score(y_eval, pred))\n",
        "    if proba is not None:\n",
        "        metrics[\"roc_auc\"] = float(roc_auc_score(y_eval, proba))\n",
        "    else:\n",
        "        metrics[\"roc_auc\"] = None # Explicitly set to None if proba is not available\n",
        "\n",
        "    # Save predictions and metrics\n",
        "    out = pd.DataFrame({\"pred\": pred})\n",
        "    if proba is not None:\n",
        "        out[\"proba\"] = proba\n",
        "    out.index = y_eval.index # Preserve original index for predictions\n",
        "    out_path = ART / f\"preds_{key}.parquet\"\n",
        "    out.to_parquet(out_path)\n",
        "    (ART / f\"metrics_{key}.json\").write_text(json.dumps(metrics, ensure_ascii=False), encoding=\"utf-8\")\n",
        "    results[key] = metrics\n",
        "    print(f\"Saved preds to: {out_path}, metrics: {metrics}\")\n",
        "\n",
        "# resumen tabular\n",
        "pd.DataFrame(results).T"
      ],
      "metadata": {
        "id": "tBqQnIlpeukv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current = list(pd.read_parquet(ART/\"df_transformed.parquet\").drop(columns=[c for c in df_trans.columns if c.lower() in (\"target\",\"label\",\"objetivo\",\"y\")], errors='ignore').columns)\n",
        "(ART/\"features_final.json\").write_text(json.dumps(current, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"features_final.json actualizado con\", len(current), \"features.\")\n"
      ],
      "metadata": {
        "id": "u7pl6SAnfTKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualización de desempeño\n",
        "\n",
        "Se incluye la curva ROC comparativa entre modelos, exportada como `roc_comparison.png`, para ilustrar la capacidad discriminativa de cada uno.\n"
      ],
      "metadata": {
        "id": "9HIfm4WzhkJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar ROC para modelos con proba (requiere matplotlib)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "for key, model in models.items():\n",
        "    preds = pd.read_parquet(ART / f\"preds_{key}.parquet\")\n",
        "    # Use y_test directly for true labels, as it's globally available and aligned\n",
        "    if \"proba\" in preds.columns and 'y_test' in globals():\n",
        "        y_true = y_test.loc[preds.index]\n",
        "        fpr, tpr, _ = roc_curve(y_true, preds[\"proba\"])\n",
        "        plt.plot(fpr, tpr, label=f\"{key} (AUC={auc(fpr,tpr):.3f})\")\n",
        "plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.title(\"ROC comparison\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART/\"roc_comparison.png\", dpi=150)\n",
        "print(\"ROC plot saved to:\", ART/\"roc_comparison.png\")"
      ],
      "metadata": {
        "id": "vE4NGf7wfSoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabla Comparativa"
      ],
      "metadata": {
        "id": "K568ZZlMfIZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumiendo results dict del paso anterior\n",
        "pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"model\"}).to_csv(ART/\"model_comparison.csv\", index=False)\n",
        "print(\"Model comparison saved to:\", ART/\"model_comparison.csv\")\n"
      ],
      "metadata": {
        "id": "NzDYhvpPfK4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparación de modelos en conjunto test\n",
        "\n",
        "Se evaluaron los modelos optimizados sobre el conjunto de test holdout. La siguiente tabla resume las métricas principales:\n",
        "\n",
        "| Modelo          | Accuracy | ROC AUC |\n",
        "|-----------------|----------|---------|\n",
        "| Random Forest   | 0.84     | 0.91    |\n",
        "| LightGBM        | 0.86     | 0.93    |\n",
        "\n",
        "La métrica principal es AUC ROC, por su robustez ante clases desbalanceadas y su capacidad para capturar discriminación global.\n"
      ],
      "metadata": {
        "id": "TO_Pk-TrhNlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección del modelo final\n",
        "\n",
        "Se selecciona **LightGBM** como modelo final por presentar el mejor desempeño en AUC ROC (0.93), manteniendo además una alta precisión. Su capacidad para manejar features numéricas y categóricas, junto con su eficiencia computacional, lo hacen adecuado para producción.\n",
        "\n",
        "El modelo ha sido serializado como `model_final.joblib` y está listo para ser interpretado con SHAP en la siguiente sección.\n"
      ],
      "metadata": {
        "id": "V5zOh88IhU9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo final elegido (ej. lightgbm)\n",
        "final_model_name = \"lightgbm\"  # o \"random_forest\", según decisión\n",
        "model_final = models[final_model_name]\n",
        "\n",
        "import joblib\n",
        "joblib.dump(model_final, ART / \"model_final.joblib\")\n",
        "print(\"Modelo final guardado como model_final.joblib\")\n"
      ],
      "metadata": {
        "id": "ytn4jg6fj8Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusión de la evaluación\n",
        "\n",
        "La evaluación en test confirma que el modelo LightGBM supera al baseline y al Random Forest en la métrica principal. Se recomienda avanzar con su interpretación y validación operativa para priorización de proyectos mineros.\n"
      ],
      "metadata": {
        "id": "9RSOYKfbhos7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Interpretabilidad y explicación\n",
        "\n",
        "En esta sección se analiza el modelo final seleccionado, con el objetivo de entender qué variables influyen más en sus decisiones y cómo lo hacen. Se aplican técnicas de interpretabilidad global y local para extraer hallazgos accionables que puedan informar decisiones estratégicas en el contexto minero.\n",
        "\n",
        "Se utilizan dos enfoques complementarios:\n",
        "\n",
        "- **Importancia global de variables**: basada en `feature_importances_` del modelo final.\n",
        "- **Explicaciones locales**: mediante valores SHAP para observaciones individuales.\n",
        "\n",
        "El análisis se realiza sobre el modelo final ya entrenado y validado en test, asegurando que las explicaciones correspondan al comportamiento real del modelo en producción.\n"
      ],
      "metadata": {
        "id": "8MBkNXZhjDnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Qu0AdMTjfhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DhAgLeL0jfa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bTNd98CWjfX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MERqxzjQjfUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ChV87XDLjfQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.2 — Carga del modelo final y cálculo de importancias globales\n",
        "from pathlib import Path\n",
        "import joblib, pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "model_path = ART / \"model_final.joblib\"\n",
        "df_trans = pd.read_parquet(ART / \"df_transformed.parquet\")\n",
        "\n",
        "# Cargar modelo final\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# Obtener nombres de features\n",
        "feat_names = getattr(model, \"feature_names_in_\", None)\n",
        "if feat_names is None:\n",
        "    # Fallback if model does not have feature_names_in_ (e.g., older sklearn versions or custom models)\n",
        "    if hasattr(model, 'n_features_in_'):\n",
        "        feat_names = [f\"f{i}\" for i in range(model.n_features_in_)]\n",
        "    else:\n",
        "        # As a last resort, use columns from df_trans, assuming alignment\n",
        "        print(\"Warning: No feature names found in model. Inferring from df_trans.\")\n",
        "        feat_names = df_trans.columns.tolist()\n",
        "\n",
        "# Obtener importancias\n",
        "if hasattr(model, \"feature_importances_\"):\n",
        "    importances = model.feature_importances_\n",
        "elif hasattr(model, \"coef_\"):\n",
        "    importances = np.abs(model.coef_).flatten()\n",
        "else:\n",
        "    raise ValueError(\"El modelo no tiene atributo de importancia disponible.\")\n",
        "\n",
        "# Construir DataFrame ordenado\n",
        "df_imp = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "print(\"Importancias de Features (Top 10):\")\n",
        "print(df_imp.head(10).to_string())\n",
        "\n",
        "# Save the feature importances to a CSV file\n",
        "df_imp.to_csv(ART / \"feature_importances.csv\", index=False)"
      ],
      "metadata": {
        "id": "BDjaWVAnjSCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importancia global de variables\n",
        "\n",
        "Se calculó la importancia global de features a partir del modelo final. Las 10 variables más influyentes son: f12, f0, f1, f55, f48, f59, f121, f4, f7, f8. Estas importancias indican qué variables el modelo usa con mayor frecuencia y mayor ganancia para dividir el espacio de decisión.\n",
        "\n",
        "\n",
        "Importante: la importancia global no informa la dirección del efecto ni su estabilidad por subgrupos; para eso usamos SHAP en la siguiente sub-sección.\n"
      ],
      "metadata": {
        "id": "g8BfRofemS-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualización barras horizontal con top 10"
      ],
      "metadata": {
        "id": "FL1E8NZjmvQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top10 = df_imp.head(10).iloc[::-1]  # invertir para plotting\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(top10['feature'], top10['importance'], color='C0')\n",
        "plt.xlabel('Importancia')\n",
        "plt.title('Top 10 features - importancia global')\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART / \"feature_importances_top10.png\", dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved:\", ART / \"feature_importances_top10.png\")\n"
      ],
      "metadata": {
        "id": "iOvAf4V1m0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparar SHAP (global + locales)\n",
        "\n",
        "Guarda artifacts: shap_summary.png y shap_local_i.png (3 ejemplos)."
      ],
      "metadata": {
        "id": "vlI4aYzIm4x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requisitos: shap instalado (pip install shap). Ejecutar en Colab si hace falta.\n",
        "import joblib, pandas as pd, numpy as np, shap\n",
        "from pathlib import Path\n",
        "ART = Path(\"data/artifacts\")\n",
        "\n",
        "model = joblib.load(ART / \"model_final.joblib\")\n",
        "X = pd.read_parquet(ART / \"df_transformed.parquet\")\n",
        "# remover target si existe\n",
        "X = X.drop(columns=[c for c in X.columns if c.lower() in (\"target\",\"label\",\"objetivo\",\"y\")], errors='ignore')\n",
        "\n",
        "# usar TreeExplainer para modelos tree-based\n",
        "explainer = shap.TreeExplainer(model)\n",
        "# sample por rendimiento si X grande\n",
        "X_sample = X.sample(n=min(200, len(X)), random_state=42)\n",
        "shap_values = explainer.shap_values(X_sample)  # para classifier, devuelve lista por clase o matriz\n",
        "\n",
        "# Determine sv (SHAP values for the positive class)\n",
        "sv = shap_values[1] if isinstance(shap_values, list) and len(shap_values) > 1 else shap_values\n",
        "\n",
        "# Determine expected_value for the positive class\n",
        "expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) and len(explainer.expected_value) > 1 else explainer.expected_value\n",
        "\n",
        "# summary plot (global)\n",
        "shap.summary_plot(sv, X_sample, show=False, plot_type=\"bar\")\n",
        "plt = __import__('matplotlib').pyplot\n",
        "plt.tight_layout()\n",
        "plt.savefig(ART / \"shap_summary.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved SHAP summary:\", ART / \"shap_summary.png\")\n",
        "\n",
        "# guardar 3 force plots locales como imágenes (ejemplo para 3 índices)\n",
        "local_idxs = list(X_sample.index[:3])\n",
        "for ii, idx in enumerate(local_idxs, start=1):\n",
        "    shap.force_plot(expected_value, sv[X_sample.index.get_loc(idx)], X_sample.loc[idx,:], matplotlib=True, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ART / f\"shap_local_{ii}.png\", dpi=150)\n",
        "    plt.close()\n",
        "    print(\"Saved SHAP local:\", ART / f\"shap_local_{ii}.png\")"
      ],
      "metadata": {
        "id": "OyPuVR0Fm70R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP summary (shap_summary.png): muestra la importancia global y la dirección del efecto.\n",
        "\n",
        "El eje horizontal indica magnitud del efecto SHAP (impacto en la predicción).\n",
        "\n",
        "Cada punto representa una observación; el color muestra el valor de la feature (rojo alto, azul bajo).\n",
        "\n",
        "Si para una feature los puntos rojos están a la derecha, valores altos de esa feature aumentan la probabilidad de la clase positiva; si están a la izquierda, la aumentan menos o la disminuyen.\n",
        "\n",
        "SHAP force plots locales (shap_local_1..3.png): explican la contribución de cada variable a la predicción de una observación individual.\n",
        "\n",
        "Elementos que empujan hacia la derecha aumentan la probabilidad; a la izquierda la reducen.\n",
        "\n",
        "Útiles para justificar casos concretos (fases del proyecto, outliers, errores de modelo)."
      ],
      "metadata": {
        "id": "yhXHLZjqnc_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checks rápidos\n",
        "Modelo usado:\n",
        "\n",
        "Confirma que data/artifacts/model_final.joblib es exactamente el modelo evaluado.\n",
        "\n",
        "Alineamiento de features:\n",
        "\n",
        "Verifica que las columnas usadas por SHAP coincidan con las columnas reales de df_transformed.parquet (mismos nombres y orden).\n",
        "\n",
        "Estabilidad de importancias:\n",
        "\n",
        "Comprueba si las top features en SHAP coinciden con feature_importances_. Si hay divergencias, prioriza SHAP para interpretar dirección y efecto.\n",
        "\n",
        "Ausencia de NaN/Inf en X_sample (ya deberías haberlo validado antes de llamar a SHAP).\n",
        "\n",
        "Reproducibilidad:\n",
        "\n",
        "Guarda seeds y el índice de X_sample usado; anota el rango de índices de las 3 observaciones locales para trazabilidad."
      ],
      "metadata": {
        "id": "dBNIDAy0nqNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import joblib, pandas as pd\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "model = joblib.load(ART/\"model_final.joblib\")\n",
        "\n",
        "# Cargar df_transformed de manera segura\n",
        "df_trans = pd.read_parquet(ART/\"df_transformed.parquet\")\n",
        "\n",
        "# detectar columnas target explícitamente (sin usar '_')\n",
        "target_cols = [c for c in df_trans.columns if c.lower() in (\"target\",\"label\",\"objetivo\",\"y\")]\n",
        "X = df_trans.drop(columns=target_cols, errors='ignore')\n",
        "\n",
        "print(\"Model feature_names sample:\", getattr(model, \"feature_names_in_\", None)[:5], \"...\")\n",
        "print(\"X columns sample:\", list(X.columns)[:10])\n",
        "print(\"Detected target columns:\", target_cols)\n"
      ],
      "metadata": {
        "id": "rgPSFaL4n9cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import joblib, pandas as pd, numpy as np, shap\n",
        "\n",
        "ART = Path(\"data/artifacts\")\n",
        "model = joblib.load(ART / \"model_final.joblib\")\n",
        "df_trans = pd.read_parquet(ART / \"df_transformed.parquet\")\n",
        "target_cols = [c for c in df_trans.columns if c.lower() in (\"target\",\"label\",\"objetivo\",\"y\")]\n",
        "X = df_trans.drop(columns=target_cols, errors='ignore')\n",
        "\n",
        "# Alinear columnas al modelo\n",
        "model_feat = list(getattr(model, \"feature_names_in_\", X.columns))\n",
        "X = X[model_feat].copy()\n",
        "\n",
        "# SHAP sobre muestra (o usa sv ya en memoria si lo tenés)\n",
        "n_sample = min(500, len(X))\n",
        "X_sample = X.sample(n=n_sample, random_state=42)\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values_all = explainer.shap_values(X_sample)\n",
        "sv = shap_values_all[1] if isinstance(shap_values_all, list) else shap_values_all\n",
        "sv = np.array(sv)\n",
        "\n",
        "rows = []\n",
        "for i, feat in enumerate(X_sample.columns):\n",
        "    abs_mean = float(np.mean(np.abs(sv[:, i])))\n",
        "    abs_med = float(np.median(np.abs(sv[:, i])))\n",
        "    xvals = X_sample[feat].values\n",
        "    # safe corr: si var(x) == 0 o var(shap) == 0 -> corr = 0\n",
        "    if np.nanstd(xvals) == 0 or np.nanstd(sv[:, i]) == 0:\n",
        "        corr = 0.0\n",
        "    else:\n",
        "        corr = float(np.corrcoef(xvals, sv[:, i])[0, 1])\n",
        "        if np.isnan(corr):\n",
        "            corr = 0.0\n",
        "    rows.append({\n",
        "        \"feature\": feat,\n",
        "        \"shap_importance_mean\": abs_mean,\n",
        "        \"shap_importance_median\": abs_med,\n",
        "        \"direction_corr\": corr,\n",
        "        \"direction_strength\": abs(corr)\n",
        "    })\n",
        "\n",
        "df_shap = pd.DataFrame(rows).sort_values([\"shap_importance_mean\",\"direction_strength\"], ascending=[False,False]).reset_index(drop=True)\n",
        "df_shap[\"rank\"] = df_shap[\"shap_importance_mean\"].rank(ascending=False, method=\"first\").astype(int)\n",
        "df_shap[\"direction\"] = df_shap[\"direction_corr\"].apply(lambda x: \"positive\" if x>0.03 else (\"negative\" if x<-0.03 else \"ambiguous\"))\n",
        "\n",
        "df_shap.to_csv(ART / \"shap_feature_table.csv\", index=False)\n",
        "df_shap.head(20)\n"
      ],
      "metadata": {
        "id": "21tP2ZpTotxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tabla interpretativa SHAP — resumen\n",
        "\n",
        "La tabla `shap_feature_table.csv` contiene para cada variable: importancia SHAP media y mediana, correlación con el valor SHAP (direction_corr) y una etiqueta de dirección (`positive`, `negative`, `ambiguous`).\n",
        "\n",
        "Interpretación rápida de las top features:\n",
        "- **f12** — mayor importancia SHAP; direction_corr positiva: valores altos de f12 tienden a aumentar la probabilidad de la clase objetivo.\n",
        "- **f55, f59, f121** — alta importancia SHAP con direction_corr negativa: valores altos disminuyen la probabilidad de la clase objetivo.\n",
        "- **f0** — importancia media y direction_corr positiva.\n",
        "- **f4, f7, f8** — importancia nula en la muestra usada; direction ambiguous (varianza nula o no informativa).\n",
        "\n",
        "Nota: la dirección se estima por correlación entre valores de la feature y sus valores SHAP en la muestra; es un proxy útil pero no reemplaza el análisis local si se detecta heterogeneidad por subgrupos.\n"
      ],
      "metadata": {
        "id": "15ppiRewozCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Conclusiones, limitaciones y recomendaciones\n",
        "\n",
        "## Conclusiones principales\n",
        "\n",
        "El modelo final (LightGBM) logró una mejora significativa respecto al baseline en la métrica principal (AUC ROC), alcanzando un valor de 0.93 en el conjunto de test. Las variables más influyentes fueron f12, f55, f59 y f0, con efectos consistentes sobre la probabilidad de pertenecer a la clase objetivo. Las explicaciones SHAP confirmaron la dirección del impacto de estas variables y permitieron validar decisiones individuales.\n",
        "\n",
        "El pipeline completo es reproducible, auditable y modular. Todos los artefactos fueron guardados en `data/artifacts/`, incluyendo modelos, métricas, predicciones y visualizaciones.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitaciones del dataset y del modelo\n",
        "\n",
        "- El dataset presenta alta colinealidad entre variables geográficas y categóricas, lo que puede afectar la estabilidad del modelo en nuevos datos.\n",
        "- Algunas variables con importancia nula o ambigua podrían ser ruido o estar mal codificadas; se recomienda revisar su origen.\n",
        "- El modelo fue entrenado sobre un conjunto limitado de observaciones (325), lo que restringe su capacidad de generalización.\n",
        "- No se incluyó validación temporal ni por región, lo que limita la robustez ante cambios operativos o geográficos.\n",
        "\n",
        "---\n",
        "\n",
        "## Recomendaciones prácticas\n",
        "\n",
        "- **Producción**: serializar el modelo final (`model_final.joblib`) junto con el preprocesador y definir un endpoint de inferencia con validación de inputs.\n",
        "- **Monitoreo**: establecer KPIs operativos como tasa de clasificación correcta, distribución de probabilidades y tasa de error por región.\n",
        "- **Pruebas adicionales**: aplicar validación cruzada estratificada por región o etapa del proyecto para detectar sesgos.\n",
        "- **Recopilación de datos**: priorizar la mejora en calidad de variables críticas (ley, mineral, ubicación) y registrar nuevos proyectos con trazabilidad completa.\n",
        "- **Interpretabilidad**: usar SHAP en producción para explicar decisiones sensibles y generar reportes por lote.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6QsFztVcpn5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Instrucciones para replicar y anexos técnicos\n",
        "\n",
        "## Reproducibilidad completa\n",
        "\n",
        "Para ejecutar el notebook en una sesión limpia:\n",
        "\n",
        "1. Clonar el repositorio o abrir el notebook en Colab.\n",
        "2. Instalar dependencias necesarias:\n",
        "   ```bash\n",
        "   pip install pandas numpy scikit-learn lightgbm shap joblib matplotlib\n"
      ],
      "metadata": {
        "id": "7LzRd_FIp6ko"
      }
    }
  ]
}